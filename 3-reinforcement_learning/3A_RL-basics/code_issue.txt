I have old code which looks like this:
import gymnasium as gym
import numpy as np

# Create the CartPole environment
env = gym.make("CartPole-v1")

# Define the new observation shape for discretization
new_observation_shape = (20, 20, 20, 20)

# Discretize the continuous state space
bins = []
for i in range(4):
    # For cart position and pole angle, use the environment's limits
    # For cart velocity and pole angular velocity, use -4 to 4 as limits
    low = env.observation_space.low[i] if (i == 0) or (i == 2) else -4
    high = env.observation_space.high[i] if (i == 0) or (i == 2) else 4
    
    # Create bins for each dimension of the state space
    item = np.linspace(low, high, num=new_observation_shape[i], endpoint=False)
    item = np.delete(item, 0)
    bins.append(item)
    print(f"Bins for dimension {i}:\n{np.around(bins[i], 2)}\n")

class QLearningAgent:
    def __init__(self, env, new_observation_shape):
        self.env = env
        self.new_observation_shape = new_observation_shape
        self.q_table = np.zeros(new_observation_shape + (env.action_space.n,))
        self.gamma = 0.99
        self.alpha = 0.1
        self.epsilon = 1
        self.epsilon_decay = self.epsilon / 4000
        self.rng = np.random.default_rng()

    def get_discrete_state(self, state):
        """Convert continuous state to discrete state"""
        return tuple(np.digitize(state[i], bins[i]) for i in range(4))

    def act(self, state, epsilon):
        """Choose action using epsilon-greedy policy"""
        if self.rng.random() > epsilon:
            return np.argmax(self.q_table[state])
        else:
            return self.rng.integers(0, self.env.action_space.n)

    def update_q_table(self, state, action, reward, next_state):
        """Update Q-table using Q-learning algorithm"""
        max_future_q = np.max(self.q_table[next_state])
        current_q = self.q_table[state][action]
        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)
        self.q_table[state][action] = new_q

    def train(self, episodes):
        reward_records = []
        early_stop_threshold = 195
        early_stop_consecutive = 100
        consecutive_solves = 0

        render_episodes = [0, 100, 1000, episodes - 1]  # Episodes to render

        for episode in range(episodes):
            state, _ = self.env.reset()
            state = self.get_discrete_state(state)
            done = False
            total_reward = 0

            # Use render_env for specific episodes
            if episode in render_episodes:
                try:
                    render_env = gym.make("CartPole-v1", render_mode="human")
                    render_state, _ = render_env.reset()
                    print(f"\nRendering episode {episode}")
                except Exception as e:
                    print(f"Error creating render environment: {e}")
                    render_env = None

            while not done:
                action = self.act(state, self.epsilon)
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                
                # Render if it's a render episode
                if episode in render_episodes and render_env is not None:
                    try:
                        render_env.render()
                        render_next_state, _, render_term, render_trunc, _ = render_env.step(action)
                        if render_term or render_trunc:
                            render_env.close()
                            render_env = None
                    except Exception as e:
                        print(f"Error rendering environment: {e}")
                        render_env = None

                done = terminated or truncated
                next_state = self.get_discrete_state(next_state)

                self.update_q_table(state, action, reward, next_state)

                state = next_state
                total_reward += reward

            # Close render_env if it's still open
            if render_env is not None:
                render_env.close()

            # Update epsilon and record rewards
            if self.epsilon - self.epsilon_decay >= 0:
                self.epsilon -= self.epsilon_decay

            reward_records.append(total_reward)
            print(f"Episode {episode} reward: {total_reward}", end="\r")

            # Check for early stopping
            if total_reward >= early_stop_threshold:
                consecutive_solves += 1
                if consecutive_solves >= early_stop_consecutive:
                    print(f"\nEnvironment solved in {episode+1} episodes!")
                    break
            else:
                consecutive_solves = 0

        print("\nTraining completed")
        return reward_records

# Create and train the agent
agent = QLearningAgent(env, new_observation_shape)
reward_history = agent.train(episodes=6_000)


And my new code looks like this:
# QLearning Agent
import numpy as np

class QLearningAgent:
    def __init__(self, action_space, state_space):
        self.action_space = action_space
        self.state_space = state_space
        self.q_table = np.zeros(state_space + (action_space.n,))
        self.gamma = 0.99
        self.alpha = 0.1
        self.epsilon = 1
        self.epsilon_decay = self.epsilon / 4_000
        self.rng = np.random.default_rng()

    def act(self, state):
        if self.rng.random() > self.epsilon:
            action = np.argmax(self.q_table[state])
            # print(f"Exploiting: state {state}, action {action}, Q-value {self.q_table[state][action]}")
        else:
            action = self.rng.integers(0, self.action_space.n)
            # print(f"Exploring: state {state}, action {action}")
        return action

    def update(self, state, action, reward, next_state, next_action=None):
        max_future_q = np.max(self.q_table[next_state])
        current_q = self.q_table[state][action]
        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)
        self.q_table[state][action] = new_q

        # print(f"Update: state {state}, action {action}, reward {reward}, next_state {next_state}")
        # print(f"Q-value changed from {current_q} to {new_q}")

        if self.epsilon - self.epsilon_decay > 0:
            self.epsilon -= self.epsilon_decay
        
        # print(f"New epsilon: {self.epsilon}")

import numpy as np
import gymnasium as gym

class CartPoleEnvironment:
    def __init__(self):
        self.env = gym.make("CartPole-v1")
        self.observation_shape = (20, 20, 20, 20)
        self.bins = self._create_bins()

    def _create_bins(self):
        bins = []
        for i in range(4):
            # For cart position and pole angle, use the environment's limits
            # For cart velocity and pole angular velocity, use -4 to 4 as limits
            low = self.env.observation_space.low[i] if (i == 0) or (i == 2) else -4
            high = self.env.observation_space.high[i] if (i == 0) or (i == 2) else 4

            # Create bins for each dimension of the state space
            item = np.linspace(low, high, num=self.observation_shape[i], endpoint=False)
            item = np.delete(item, 0)
            bins.append(item)
            # print(f"Bins for dimension {i}:\n{np.around(bins[i], 2)}\n")
        return bins

    def _get_discrete_state(self, state):
        discrete_state = tuple(np.digitize(state[i], self.bins[i]) for i in range(4))
        # print(f"Continuous state: {state}, Discrete state: {discrete_state}")
        return discrete_state

    def reset(self):
        state, _ = self.env.reset()
        return self._get_discrete_state(state), {}

    def step(self, action):
        next_state, reward, terminated, truncated, info = self.env.step(action)
        discrete_next_state = self._get_discrete_state(next_state)
        return discrete_next_state, reward, terminated, truncated, info

    def render(self):
        self.env.render()

    def close(self):
        self.env.close()

# Create the environment and agent
env = CartPoleEnvironment()
agent = QLearningAgent(env.env.action_space, env.observation_shape)

# define train_agent function

def train_agent(agent, env, num_episodes=2):
    reward_history = []
    early_stop_threshold = 195
    early_stop_consecutive = 100
    consecutive_solves = 0

    for episode in range(num_episodes):
        state, _ = env.reset()
        total_reward = 0
        done = False

        while not done:
            action = agent.act(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            agent.update(state, action, reward, next_state)
            state = next_state
            total_reward += reward

        reward_history.append(total_reward)
        print(f"Episode {episode} reward: {total_reward}", end="\r")

        if total_reward >= early_stop_threshold:
            consecutive_solves += 1
            if consecutive_solves >= early_stop_consecutive:
                print(f"\nEnvironment solved in {episode+1} episodes!")
                break
        else:
            consecutive_solves = 0

    print("\nTraining completed")
    return reward_history


# Train the agent
reward_history = train_agent(agent, env)


---

As you can see, I cleaned it up quite a bit. But it should be eseentially the same. However, the old code trains and learns a good policy whereas the new code trains but fails to learn anything at all. Can you spot any issues? Where are teh differences? I'm so lost. Please help me.