{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3bkcStgj37A"
      },
      "source": [
        "# **BAYESIAN MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObtAcirQEinz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import scipy.stats as sps\n",
        "import scipy.integrate as spi\n",
        "import scipy.special as spsc\n",
        "import scipy.optimize as spo\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUFHA2h7l2hP"
      },
      "source": [
        "## **Contents**\n",
        "\n",
        "1. Basic bayes\n",
        "\n",
        "      a. Bayes theorem\n",
        "\n",
        "      b. Bayesian statistics\n",
        "\n",
        "      c. Using prior knowledge\n",
        "\n",
        "\n",
        "2. Bayesian observers\n",
        "\n",
        "      a. Neural orientation likelihoods\n",
        "\n",
        "      b. Cardinal prior\n",
        "\n",
        "      c. Biological implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXFCxjmCm8rR"
      },
      "source": [
        "## Part 1: Basic Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jBkgFM3nIDY"
      },
      "source": [
        "### 1a. Bayes Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K7NQd8cncFI"
      },
      "source": [
        "Bayes' theorem looks rather simple:\n",
        "\n",
        "\\begin{align}\n",
        " p(A|B) = \\frac{p(B|A) p(A)} {p(B)}\n",
        "\\end{align}\n",
        "\n",
        "\\\\\n",
        "The probability of A given B is equal to the probability of B given A multiplied by the probability of A, divided by the probability of B. But many people find this unintuitive, generating what's known as base-rate neglect (Kahneman & Tversky, 1973). The typical example is:\n",
        "\n",
        "*Suppose that 0.1% of all people in a population carry a virus. A\n",
        "diagnostic test for this virus detects it in 100% of the people who have the virus, but also gives false alarms on 5% of the people who do not have the virus. What is the chance that a person with a positive test result actually has the virus?*\n",
        "\n",
        "In a recent experiment (Stengard et al., Cognition, 2022) found the modal response was \"95%\" (~20% of participants).\n",
        "\n",
        "Calculate the correct response using Bayes Theorem. Use A = \"having the virus\" and B = \"testing positive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oM2i0H4jqdG8"
      },
      "outputs": [],
      "source": [
        "# Define pA, the probability of having the virus\n",
        "\n",
        "\n",
        "# Define pBA, the probability of testing positive given you have the virus\n",
        "\n",
        "\n",
        "# Define pB, the probability of testing positive\n",
        "# This is the sum of the 0.1% of the population who have the virus and will\n",
        "# test positive, plus the 99.9% of the population who have a 5% chance of\n",
        "# testing positive\n",
        "\n",
        "\n",
        "# Apply Bayes theorem, pAB = (pBA*pA)/pB and print the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJk6wrZcqj3d"
      },
      "source": [
        "Stengard et al., found just 9% of participants gave an answer between 1.8% and 2.2%, in line with previous studies, where the correct answer is given less than 20% of the time.\n",
        "\n",
        "If you found that unintuitive, try another (Based on Bar-Hillel, 1980):\n",
        "\n",
        "*In the city, 85% of cabs are blue and 15% are green. In a hit and run accident, an eye-witness identifies a green cab. The accident happened at night, and when tested, it was found the witness confuses blue and green at night 20% of the time. What is the chance the cab was green?*\n",
        "\n",
        "A = \"cab is green\", B = \"witness sees green\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hq656PP_q-2Q"
      },
      "outputs": [],
      "source": [
        "# Define pA, the probability that the cab is green\n",
        "\n",
        "\n",
        "# Define pBA, the probability that the witness reports a green cab is green\n",
        "\n",
        "\n",
        "# Define pB, the probability that the witness reports a green cab, given either\n",
        "# a green or a blue cab\n",
        "\n",
        "\n",
        "# Apply Bayes and print the result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct answer: 41.38%"
      ],
      "metadata": {
        "id": "TMdGIT1dqtEe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2VGshiDrDGZ"
      },
      "source": [
        "### 1b. Bayesian Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5zQIByOrMLK"
      },
      "source": [
        "The terms of Bayes theorem are often referred to as:\n",
        "- the posterior $p(A|B)$,\n",
        "- the likelihood $p(B|A)$,\n",
        "- the prior $p(A)$, and\n",
        "- the marginal likelihood $p(B)$.\n",
        "\n",
        "Here we use the term 'likelihood'.\n",
        "\n",
        "Note that probabilities have to add to 1: $p(A) = 1 - p({\\displaystyle \\neg }A)$, likelihoods do not.\n",
        "\n",
        "In Bayesian statistics, we think about this in terms of data (e.g., a random variable $\\boldsymbol{x}$) and model parameters $\\theta$: We ask, *what is the probability of the model parameter, given the data, $p(\\theta | \\boldsymbol{x})$?*\n",
        "\n",
        "For example, what is the probability of the null hypothesis that the mean is 0? Which is calculated as the probability of the data given the model parameter $p(\\boldsymbol{x}|\\theta)$, multiplied by the prior probability of model parameter $p(\\theta)$. Therefore, Bayes theorem in the context of parameter estimation is\n",
        "\n",
        "\\begin{align}\n",
        " p(\\theta|\\boldsymbol{x}) = \\frac{p(\\boldsymbol{x}|\\theta) p(\\theta)} {p(\\boldsymbol{x})}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "Simple example: We observe N = 50 samples of data $\\boldsymbol{x}$, which have been sampled from a Gaussian with a true mean $\\mu = 0$ and standard deviation $\\sigma = 1$: $\\boldsymbol{x} \\sim \\mathcal{N}(0, 1)$. Our parameters are $\\theta = \\{\\mu, \\sigma \\}$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "Use `sps.norm.rvs(loc=mu, scale=sigma, size=N)` to take 50 samples of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv6MCtQZsrqQ"
      },
      "outputs": [],
      "source": [
        "# take 50 random samples from a Normal distribution\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQtRISB0tGJU"
      },
      "source": [
        " Assume we know the true standard deviation is 1, but we're unsure about  the true mean: it could be anywhere between -1 and 1, with equal likelihood (uniform/uninformative prior)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71p5O968tMDJ"
      },
      "outputs": [],
      "source": [
        "# Prior function: A function of mu that uses sps.uniform.pdf to calculate the\n",
        "# likelihood of mu under the prior uniform distribution; location = -1, scale = 2\n",
        "def uniform_prior(mu):\n",
        "\n",
        "    prior = sps.uniform.pdf(x=mu, loc=-1, scale=2)\n",
        "\n",
        "    return prior\n",
        "\n",
        "\n",
        "# Likelihood function: returns the likelihood of the data given mu. The input\n",
        "# The likelihood function is a normal distribution: sps.norm.pdf with scale\n",
        "# (sigma) fixed to 1 (because we are assuming we know the true standard deviation)\n",
        "def likelihood(data, mu):\n",
        "\n",
        "    lh = sps.norm.pdf(x=data, loc=mu, scale=1)\n",
        "\n",
        "    return lh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb_CFJ1xt5_7"
      },
      "source": [
        "The posterior can be calculated as proportional to the prior multiplied by the likelihood: $p(\\theta | \\boldsymbol{x}) \\propto p(\\theta) p(\\boldsymbol{x} | \\theta)$.\n",
        "\n",
        "For now, ignore the marginal likelihood $p(\\boldsymbol{x})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiKcFFTJuEUs"
      },
      "outputs": [],
      "source": [
        "# Define a function to return the posterior:\n",
        "# uniform_prior(mu) * likelihood(data, mu)\n",
        "# Hint: the posterior is the product over the data samples\n",
        "\n",
        "def posterior_prod(mu, data):\n",
        "\n",
        "    post = uniform_prior(mu) * likelihood(data, mu)\n",
        "\n",
        "    return np.prod(post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxLJAsT-wvGK"
      },
      "source": [
        "To visualise, make a histogram of the data and overlay distributions with 10 possible means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No1JAk8Uxb1W"
      },
      "outputs": [],
      "source": [
        "# Use np.linspace to take 5 possible means between -1 and 1\n",
        "\n",
        "\n",
        "# For the x-axis, take 100 possible values between -4 and 4\n",
        "\n",
        "\n",
        "# Plot a histogram of the observed data (use density=True)\n",
        "plt.figure(figsize=(3,2), dpi=100)\n",
        "\n",
        "\n",
        "\n",
        "# Overlay the expected distributions for each of the 5 possible mus\n",
        "# You can use the likelihood function with data=xs (the x-axis values)\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel('Data value')\n",
        "plt.ylabel('Density')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3wzRI8z-2S"
      },
      "source": [
        "Plot the prior, the likelihood, and the posterior, over 100 discrete possible means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aASSpLH0LIb"
      },
      "outputs": [],
      "source": [
        "# Define a new x-axis, which is 100 possible mus evenly spaced between -1 and 1\n",
        "mus = np.linspace(-1, 1, 100)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax = fig.subplots(3, 1)\n",
        "# On  subplot 1, plot the uniform prior function of mu\n",
        "\n",
        "\n",
        "\n",
        "ax[0].set(xlabel='mu', ylabel='prior')\n",
        "\n",
        "# On a second subplot, plot the likelihood of the data as a function of mu\n",
        "# The likelihood is the product across data samples, so take the output of the\n",
        "# likelihood function as a numpy array\n",
        "\n",
        "\n",
        "\n",
        "ax[1].set(xlabel='mu', ylabel='likelihood')\n",
        "\n",
        "# Calculate the posterior, using the functions, for each possible mu\n",
        "post = np.array([posterior_prod(m, data) for m in mus])\n",
        "\n",
        "# Plot on the third subplot\n",
        "\n",
        "ax[2].set(xlabel='mu', ylabel='posterior')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqIvPZz91q2b"
      },
      "source": [
        "What is the posterior probability of the null hypothesis that the mean\n",
        "is 0?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgMMO3Z41yX6"
      },
      "outputs": [],
      "source": [
        "# Define pH0, the value of the posterior at mean = 0 and print the value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIkGNaAS2AXO"
      },
      "source": [
        "What is the posterior probability of alternative hypothesis 1: the mean is greater than 0 (up to 1)?\n",
        "\n",
        "For this, we could do some math (integrate over values from 0 to 1), but in reality we're often faced with more complicated problems that don't have simple mathematical solutions... We can use numerical integration to estimate the probability density under the posterior from 0 to 1. Use the function `spi.quad(function, a, b)` to integrate the posterior from a=0 to b=1. An easy way to pass the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkPiIN152fPe"
      },
      "outputs": [],
      "source": [
        "# Calculate pH1 using spi.quad, and the posterior function we already defined\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KEg7N2E2q7s"
      },
      "source": [
        "Model comparison can be performed using Bayes factor:\n",
        "\n",
        "\\begin{align}\n",
        "BF = \\frac{p(H_0)}{p(H_1)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEirbATz3QDs"
      },
      "outputs": [],
      "source": [
        "# Calculate BF and print the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkD2l0nD3UCH"
      },
      "source": [
        "If this kind of comparison is our goal, we can safely ignore the\n",
        "marginal likelihood term, probability of the data $p(\\boldsymbol{x})$, because it is the same for $p(H_0)$ and $p(H_1)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-G0MMh3q9Y"
      },
      "source": [
        "### 1c. Using Prior Knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAEVlUd302X"
      },
      "source": [
        "What if we have some idea that the mean should be close to 0?\n",
        "\n",
        "We can set the prior to a normal distribution, with a mean of 0 and a stantard deviation of 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc622V_X397F"
      },
      "outputs": [],
      "source": [
        "# Define a new function for the prior, which is a gaussian distribution\n",
        "# over possible mus, with location = 0 and scale = 0.2\n",
        "\n",
        "\n",
        "\n",
        "# Define a new function for the prosterior, which is the same as before,\n",
        "# but uses the new gaussian prior\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQp52qio4FdZ"
      },
      "source": [
        "To visualise, plot the two priors, likelihood, and the two different posteriors in the same manner as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hykBjJpW4HYz"
      },
      "outputs": [],
      "source": [
        "# Make a new figure with 3 subplots\n",
        "\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax = fig.subplots(3, 1)\n",
        "\n",
        "# On the first subplot, plot the uniform prior as before, and the new gaussian prior\n",
        "# (Use different colours)\n",
        "\n",
        "\n",
        "ax[0].set(xlabel='mu', ylabel='prior')\n",
        "\n",
        "# On the second subplot, plot the likelihood as before\n",
        "\n",
        "\n",
        "ax[1].set(xlabel='mu', ylabel='likelihood')\n",
        "\n",
        "# plot the previous posterior, and overlay the new posterior\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# use the right-hand y-axis, as there is a big difference in the posterior\n",
        "\n",
        "\n",
        "ax[2].set(xlabel='mu', ylabel='posterior')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SThHEMzjEin-"
      },
      "outputs": [],
      "source": [
        "# what is the likelihood of the null hypothesis that the mean is 0?\n",
        "\n",
        "\n",
        "\n",
        "# Use spi.quad to estimate the likelihood of the alternative hypothesis as before\n",
        "\n",
        "\n",
        "# Print Bayes Factor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XrAzOp652-c"
      },
      "source": [
        "You should see that the posterior is now closer to 0 and has a tighter range: using a more specific prior increases accuracy and decreases uncertainty (so long as the prior is valid).\n",
        "\n",
        "Bayes Factor is also much larger, indicating greater evidence in favour of the null hypothesis.\n",
        "\n",
        "If you want, try changing the prior to be centred on mu = 0.1, and see what happens to the posterior and Bayes Factor.\n",
        "\n",
        "\\\\\n",
        "\n",
        "Things can get more complicated: we're often dealing with uncertainty across multiple parameters. Say we didn't know the standard deviation, then we would have to marginalise this out to look at the posterior mean. Marginalising is another integral, we would end up with:\n",
        "\n",
        "\\begin{align}\n",
        "p(H_1|\\boldsymbol{x}) = \\int{ \\int{ p(\\boldsymbol{x}|\\mu,\\sigma)p(\\sigma)p(\\mu)d\\sigma} d\\mu}\n",
        "\\end{align}\n",
        "\n",
        "As things get more complicated, we can switch the MCMC to estimate the integral. This is a problem for another day.\n",
        "\n",
        "A great introduction to the MCMC approach:\n",
        "van de Schoot, R., Depaoli, S., King, R., Kramer, B., Märtens, K., Tadesse, M. G., ... & Yau, C. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1), 1.\n",
        "\n",
        "More on Bayesian hypothesis testing:\n",
        "Kruschke, J. (2014). Doing Bayesian data analysis. A tutorial with R:\n",
        "JAGS, and Stan. Elsevier Science.\n",
        "AKA \"The puppy book\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKLpamKTEZDE"
      },
      "source": [
        "## Part 2: Bayesian Observers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfG-hDKLEh-b"
      },
      "source": [
        "### 2a. Neural Orientation Likelihoods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aiz2l8zEo2s"
      },
      "source": [
        "Bayes is the ***'optimal'*** way to use what we know about previous 'states of the world' to have better estimates of current 'states of the world', as well as reducing our uncertainty.\n",
        "\n",
        "Perception is a fundamental example of a useful application of Bayes: we do not have direct access to any 'truth' of the world around us, only noisy estimates from indirect measures (light, sound, et cetera).\n",
        "\n",
        "Perception is the result of a decision about the most likely cause of the noisy and ambigous sensory evidence, which is additionally disrupted by our own noisy perceptual processes.\n",
        "\n",
        "Using Bayes, this can be framed as taking the maximum likelihood of our\n",
        "posterior estimate of the cause of our sensations:\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{align}\n",
        "p(cause | sensations) = \\frac{p(sensations | cause)p(cause)} { p(sensations)}\n",
        "\\end{align}\n",
        "\n",
        "\\\\\n",
        "\n",
        "A basic example is in perceiving the orientation of a line.\n",
        "\n",
        "V1, the first cortical step of visual processing, contains neurons whose firing rates are a function of orientation. Specifically, firing rate is modulated according to a circular gaussian (Von Mises) distribution, with a certain mean, mu, and concentration, kappa (~ inverse variance).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih57f082Ein_"
      },
      "outputs": [],
      "source": [
        "# Von Mises Distribution:\n",
        "# x and mu are in radians (not degrees)\n",
        "def vmdist(x, kappa, mu):\n",
        "\n",
        "    pdf = (1 / (np.pi * spsc.i0(kappa))) * np.exp(kappa * np.cos(2 * (x - mu)))\n",
        "\n",
        "    return pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's pretend V1 has 18 neurons, whose firing rates are tuned according to distributions with means equidistant across the full 180 degrees of possible orientations.\n",
        "\n",
        "Plot the firing rates of the 18 neurons as a function of presented orientation"
      ],
      "metadata": {
        "id": "t3-8CoM4776l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXnshJCQFkGu"
      },
      "outputs": [],
      "source": [
        "nneurons = 18\n",
        "\n",
        "# Define v1mus as linearly spaced between 0 and pi radians (180 degrees)\n",
        "\n",
        "\n",
        "# Von Mises concentration parameter\n",
        "kappa = 3.65  # estimating fwhm of about 70 degrees, similar to real V1 neurons\n",
        "\n",
        "# To plot, use an even sampling of 180 possible presented orientations, between 0 and pi\n",
        "# Define oris, the samples of possible presented orientations (in radians)\n",
        "\n",
        "\n",
        "# Create an array of the firing rates as a function of presented orientations\n",
        "# for the 18 neurons, where each row is a neuron\n",
        "# Use the Von Mises distribution function defined above\n",
        "\n",
        "\n",
        "# Plot the firing rate of each neuron as a function of presented orientation\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "\n",
        "ax.set(xlabel='Presented Orientation', ylabel='Firing rate')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keTLhpwvGxsC"
      },
      "source": [
        "Neuron spike counts tend to be poisson distributed, the plot shows something proportional to mean firing rate (in response to each presented orientation), the mean is also proportional to the variance (because it's a poisson).\n",
        "\n",
        "You should see from the plot that the firing rate of any one neuron is ambiguous with respect to which orientation was presented. For example, if the neuron maximally tuned to 90 degrees (cyan if using default colours) were firing at 0.8 of its maximal rate, this could be due to a presented orientation of either ~80 degrees or ~110 degrees.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log-likelihood of the stimulus orientation given the neuron's spike count can be estimated as the spike count multiplied by the log of the firing rate function (Seung & Sompolinsky, 1993; Jazayeri & Movshon, 2006).\n",
        "\n",
        "The estimate of the orientation will be the maximum a posteriori likelihood (**MAP**).\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "Note: working with log probability is easier than working with probability\n",
        "\n",
        "Reminder:\n",
        "\n",
        "$log(p(A)p(B)) = log(p(A)) + log(p(B))$\n",
        "\n",
        "$log(p(A)/p(B)) = log(p(A)) - log(p(B))$\n",
        "\n",
        "\\\\\n",
        "\n",
        "So if we want to calculate the log-likelihood of the presented orientation given the response of all neurons we can *sum* the log likelihoods across all neurons.\n"
      ],
      "metadata": {
        "id": "s-j_iHlE-wVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t218clKEioI"
      },
      "outputs": [],
      "source": [
        "# We are going to estimate the MAP orientation based on the average\n",
        "# firing rates of our 18 neurons, in response to 10 test orientations\n",
        "ntest = 10\n",
        "\n",
        "# Define the test orientations as linearly spaced between 0 and pi\n",
        "\n",
        "\n",
        "\n",
        "# The average posterior log likelihood is the log of the firing rate function\n",
        "# multiplied by the firing rate, summed over the neurons\n",
        "# Calculate the averate posterior log likelihood for each presented test orientation\n",
        "# (the posterior log likelihood distributed over the possible orientations defined above)\n",
        "\n",
        "\n",
        "# Find the location of the maximum posterior log likelihood for each test orientation\n",
        "# using np.argmax\n",
        "\n",
        "\n",
        "# The average MAP orientation is the possible orientation corresponding to\n",
        "# the location of the MAP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNXrX2D4MuwT"
      },
      "source": [
        "Plot the average posterior log likelihood for each test orientation and scatter the maximum a posteriori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChBvz7y8NEzK"
      },
      "outputs": [],
      "source": [
        "# Create a figure\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# For each test orientation\n",
        "\n",
        "    # Plot the average posterior log likelihood across possible orientations\n",
        "\n",
        "    # scatter the average MAP orientation by max post log likelihood\n",
        "\n",
        "    # for comparison, plot lines showing the test orientations\n",
        "\n",
        "\n",
        "ax.set(xlabel='Orientation', ylabel='Log posterior')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhQq4M7eP6PQ"
      },
      "source": [
        "This is the expected average MAP, given the firing rate functions.\n",
        "\n",
        "We can also simulate the neuron firing rates, to examine variability in the MAP orientation over multiple presentations of the test orientations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhbK9sr0sYCW"
      },
      "outputs": [],
      "source": [
        "# General function to get simulated neural MAP\n",
        "def simMAP(test_oris,possible_oris,v1mus,kappa,v1fr,prior,nsim):\n",
        "\n",
        "    nneurons = v1mus.shape[0] # the number of neurons\n",
        "    ntest = test_oris.shape[0] # the number of test orientation\n",
        "\n",
        "    MAP = np.zeros((ntest,nsim))\n",
        "\n",
        "    # for each test orientation\n",
        "    for ti in range(ntest):\n",
        "\n",
        "      # posterior log likelihood is summed over all neurons\n",
        "      post = np.zeros((nsim, 180))\n",
        "\n",
        "      for ni in range(nneurons):\n",
        "\n",
        "          # the mean firing rate in response to the test orientation\n",
        "          fr = vmdist(test_oris[ti], kappa, v1mus[ni])\n",
        "\n",
        "          # The simulated firing rates, drawn from a poisson with mean = fr\n",
        "          post += sps.poisson.rvs(fr, size=(nsim, 1)) * np.log(v1fr[ni]) + np.log(prior)\n",
        "\n",
        "      # get the MAP orientations for each test orientation\n",
        "      MAP[ti] = oris[np.argmax(post, axis=1)]\n",
        "\n",
        "\n",
        "    return MAP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this function, assuming a uniform prior over the possible orientations, to generate 100 simulated MAP orientations.\n"
      ],
      "metadata": {
        "id": "8rKB4atnQ_pk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTdYK3eHQcSc"
      },
      "outputs": [],
      "source": [
        "# Define the prior as equiprobable across the 180 possible orientations\n",
        "\n",
        "\n",
        "# use the function above to generate 100 simulate MAP orientations for each\n",
        "# test orientation\n",
        "\n",
        "\n",
        "# Create a scatterplot of the actual test orientations against the simulated MAP\n",
        "# orientations\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "# also plot the average of the simulated MAP orientations\n",
        "\n",
        "\n",
        "ax.set(xlabel='True orientation', ylabel='Inferred orientation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7x6TmLjSg7a"
      },
      "source": [
        "### 2b. Cardinal Prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltogQHmCTE9n"
      },
      "source": [
        "In the real world, we are more likely to encounter orientations close vertical and horizontal (the 'Cardinal' orientations; Girshick, Landy, & Simoncelli, 2011).\n",
        "\n",
        "\n",
        "This prior can be roughly described by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "W1T4aqXdTPo3",
        "outputId": "13536271-fc91-4d4f-9b14-b7770728fe30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7cee74d41510>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Linear spacing of possible orientations\n",
        "oris = np.linspace(0, np.pi, 180)\n",
        "# The prior peaks at 0, 90, and 180 degrees (horizontal and vertical)\n",
        "prior_ori = (1 - np.abs(np.sin(2*oris)) * 0.3) + 0.01\n",
        "\n",
        "plt.figure(figsize=(3,2), dpi=100)\n",
        "plt.plot(oris*180/np.pi,prior_ori, 'k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jpDYQpMTW4t"
      },
      "source": [
        "We can look at the effect of this prior on the average estimate by adding it to the mean posterior (multiplied by the number of neurons)\n",
        "since $log(p(A)*p(B)) = log(p(A)) + log(p(B))$\n",
        "\n",
        "Add the prior, recalculate the average MAP and plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQtm27JVEioI"
      },
      "outputs": [],
      "source": [
        "# Define a second average posterior log likelihood by adding the log of the\n",
        "# prior likelihood (x nneurons) to the average posterior calculated earlier\n",
        "\n",
        "\n",
        "# Calculate the new MAP orientation, using the armax location of the MAP as before\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the new posterior likelihood functions for each test orientation as before, scattering the MAP and marking the true test orientations.\n",
        "\n",
        "It might help to visualise the log prior and the firing rates in subplots above this."
      ],
      "metadata": {
        "id": "MXT1IaJBVULC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNGKjjBBTeDG"
      },
      "outputs": [],
      "source": [
        "# Plot the log prior in the first subplot\n",
        "# and the firing rates in the second\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.subplots(3, 1)\n",
        "\n",
        "# plot prior\n",
        "\n",
        "\n",
        "# plot fr\n",
        "\n",
        "ax[1].set(xlabel='Orientation', ylabel='firing rate')\n",
        "\n",
        "# plot log posterior for each test orientation,\n",
        "\n",
        "    # plot the posterior log likelihood\n",
        "\n",
        "\n",
        "    # scatter the MAP orientation\n",
        "\n",
        "\n",
        "    # plot the true test orientations for comparison\n",
        "\n",
        "\n",
        "ax[2].set(xlabel='Orientation', ylabel='log posterior')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeLEhYH4T_Hw"
      },
      "source": [
        "You should see that the estimated orientations are pulled toward the cardinal orientations. Why would this be helpful?\n",
        "\n",
        "Simulate again, and plot the estimated against the true orientation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPiVGeJqEioJ"
      },
      "outputs": [],
      "source": [
        "# Use the simMAP function again to simulate 100 MAP orientations resulting the\n",
        "# test orientations with this new prior\n",
        "\n",
        "\n",
        "# Scatter the simulations against the true orientation\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ax.set(xlabel='True orientation', ylabel='Inferred orientation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POfll9L3UtZu"
      },
      "source": [
        "You should see both a bias toward horizontal (0/180 degrees) and vertical (90 degrees), as well as the estimates being more precise around horizontal/vertical (less spread in the scatter points). This means that perception is more precise for orientations we encounter more often.\n",
        "\n",
        "These are both properties reflected in human behavioural responses to\n",
        "orientations (Wei & Stocker, 2015) and in BOLD responses to orientations (Patten, Mannion, & Clifford, 2017). (These two references also point to a more complex implementation via principles of 'efficient encoding', for those interested to read further).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zey5chTzU1SZ"
      },
      "source": [
        "### 2c. Biological implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpkPRKfVDKI"
      },
      "source": [
        "How is this implemented in the brain? Do we actually track and represent priors? Or is there some way this can\n",
        "be implemented in a feedforward manner?\n",
        "\n",
        "Re-arrange the neurons so that they are spaced according to the prior (a bit more dramatic than the current prior), and plot the firing rate functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAI89yZxVLhV"
      },
      "outputs": [],
      "source": [
        "# To space the neurons according to the prior, we can create a cumulative probability function of the prior\n",
        "thisp = np.cumsum((1 - np.abs(np.sin(2*oris)) * 0.8)) / np.sum((1 - np.abs(np.sin(2*oris)) * 0.8))\n",
        "\n",
        "# And find samples equidistant in cumulative probability\n",
        "ip = np.linspace(0.001, 0.999, nneurons)\n",
        "\n",
        "# Place the prior means about there\n",
        "v1mus_prior = np.zeros(nneurons)\n",
        "for ni in range(nneurons):\n",
        "    thisi = np.where(thisp >= ip[ni])[0][0]\n",
        "    v1mus_prior[ni] = oris[thisi]\n",
        "\n",
        "# Create a new array of firing rates for these neurons\n",
        "\n",
        "\n",
        "# plot the firing rate over possible orientations for each neuron\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "\n",
        "ax.set(xlabel='Orientation', ylabel='Firing rate')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYsjLyh5WhWe"
      },
      "source": [
        "Recalculate the mean posterior, simulate responses and plot (as above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cp9xRkgEioK"
      },
      "outputs": [],
      "source": [
        "# The average posterior log likelihood is the log firing rate (of the prior\n",
        "# distributed neurons) multiplied by the average firing rate in response to the\n",
        "# test orientations, summed across neurons\n",
        "\n",
        "\n",
        "# The MAP orientations, based on the location of the max posterior\n",
        "\n",
        "\n",
        "# plot the postior log likelihood and MAP orientations as before\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ax.set(xlabel='orientation', ylabel='log posterior')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SteWrxXrEioK"
      },
      "outputs": [],
      "source": [
        "# Use the simMAP function again to simulate 100 MAP orientations resulting the\n",
        "# test orientations with the neurons distributed this way, but inputting a\n",
        "# uniform / uninformative prior\n",
        "prior = np.ones((1,180))/180\n",
        "\n",
        "\n",
        "# scatter the MAP orientations against the true orientations as before\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ax.set(xlabel='True orientation', ylabel='Inferred orientation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBnRFMeWXR7o"
      },
      "source": [
        "You should see that having more neurons with tuning functions closer to horizontal/vertical has the same effect as the cardinal prior. The prior is now already built-in to the neural response.\n",
        "\n",
        "Early work examining cat visual cortex has demonstrated a profound impact of early visual experience on the distribution of neuronal orientation tuning functions as well as sensitivity to orientations (Blackmore & Cooper,\n",
        "1970; Hirsch & Spinelli, 1970; Hubel & Weisel, 1961).\n",
        "\n",
        "An oversimplification: the brain builds priors into the neural tuning functions naturally, by developing more neural resources for operations that are used more often (\"neurons that fire together wire together\").\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}