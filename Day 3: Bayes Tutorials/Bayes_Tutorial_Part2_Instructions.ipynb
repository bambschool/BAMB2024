{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tONB8T9wBfmu"
      },
      "source": [
        "# Bayes tutorial Part 2: Bayesian Decision Theory\n",
        "\n",
        "References:\n",
        "* The ultimate reference (highly recommend): [Bayesian models of action and perception](https://www.cns.nyu.edu/malab/bayesianbook.html)\n",
        "* Primer on Bayesian Decision Models & fitting them to data: [Ma '19\n",
        "](https://www.cell.com/neuron/pdfExtended/S0896-6273(19)30840-2)\n",
        "* Primer on the conections between Bayesian Decision Theory & RL: [Dayan & Daw '08](https://www.princeton.edu/~ndaw/dd08.pdf)\n",
        "* Past tutorials that have inspired this one: [Hermundstad Cosyne '20](https://www.janelia.org/sites/default/files/Labs/Hermundstad%20Lab/Slides_Cosyne2020.pdf), [Ma Cosyne '19](http://www.cns.nyu.edu/malab/static/files/courses/Bayesian/Slides.pdf)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i0___PHqBfmw"
      },
      "source": [
        "### This part of the tutorial has 2 (+1 bonus) subsections\n",
        "\n",
        "1. **Perceptual decisions**: Take what you have learnt about Bayesian perceptual inference in Part 1, and use it to make decisions about a stimulus\n",
        "    1. In the face of unequal priors\n",
        "    2. In the face of unequal costs/rewards - introducing the concept of *utility*\n",
        "\n",
        "<br>\n",
        "\n",
        "2. **Hierarchical inference**: Add an extra level of Bayesian inference when there are more hidden variables of interest (e.g. an underlying context)\n",
        "    1. Inferring (hidden) context by observing spikes - introducing the concept of *marginalization*\n",
        "    2. In the face of changing contexts\n",
        "\n",
        "    \n",
        "\n",
        "##### BONUS:\n",
        "\n",
        "3. **Reinforcement learning**: Turn this into an RL problem when costs/rewards are unknown, so you have to additionally *learn them*\n",
        "    1. Markov decision process & its connections to RL\n",
        "    2. In the face of unknown costs/rewards - introducing the concept of *value*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import relevant modules and define some plotting functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm, vonmises\n",
        "import scipy.optimize as opt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from typing import Dict, List\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_context('talk')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HtNv_wLjJPFt"
      },
      "source": [
        "# Problem setup\n",
        "\n",
        "**We are going to consider the decision processes of a virtual bee foraging in a virtual land with patches of two types of environments (or contexts $c$): \"forests\" and \"fields\".** \n",
        "\n",
        "The two contexts differ in the kind of flowers they have. We will refer to the flower type as $f$, you can think of it as a continuous variable parametrizing flower colors. We will assume that the flower colors $f$ in the two contexts are normally distributed but differ in their means. \n",
        "\n",
        "$$ p(f | c = \\text{forest})  = \\mathcal{N}(f; \\mu_{\\text{forest}}, \\sigma^2)$$\n",
        "\n",
        "$$ p(f | c = \\text{field})  = \\mathcal{N}(f; \\mu_{\\text{field}}, \\sigma^2)$$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So first, let's setup a class with some methods that will let us easily create and sample an encountered flower color $f$ from the two contexts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXkavKP5Bfmw"
      },
      "outputs": [],
      "source": [
        "class Context:\n",
        "    \n",
        "    # initialize\n",
        "    def __init__(self, pFlowerMu, pFlowerSig = 0.1):\n",
        "        self.pFlowerMu = pFlowerMu   # mean of the flower color distribution\n",
        "        self.pFlowerSig = pFlowerSig  # standard deviation of flower color distribution\n",
        "\n",
        "    # method 1: Probability density of flower colors \n",
        "    def pFlower(self, f):\n",
        "        return norm.pdf(\n",
        "            f, \n",
        "            loc = self.pFlowerMu, \n",
        "            scale = self.pFlowerSig)\n",
        "\n",
        "    # method 2: Cumulative density of flower colors \n",
        "    def cFlower(self, f):\n",
        "        return norm.cdf(\n",
        "            f,\n",
        "            loc = self.pFlowerMu, \n",
        "            scale = self.pFlowerSig)\n",
        "\n",
        "    # method 3: Randomly sample a flower color \n",
        "    def sample(self, n = 1):\n",
        "        return norm.rvs(\n",
        "            loc = self.pFlowerMu, \n",
        "            scale = self.pFlowerSig,\n",
        "            size = n)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6GWKMlQRBfmx"
      },
      "source": [
        "Now with this class, let's create the two contexts,  $c \\in \\{\\text{forests},\\text{fields}\\}$ which differ in the average color of flowers $f$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "0AWGGGRYBfmx",
        "outputId": "1afceee9-d0b7-4b04-f240-458c047ca923",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# make a dictionary for easily referring to the two contexts\n",
        "contexts = {\n",
        "    'forest': Context(pFlowerMu = 0.66), \n",
        "    'field': Context(pFlowerMu = 0.33) \n",
        "    }\n",
        "\n",
        "# define some useful variables for plotting\n",
        "colors = {\n",
        "    'forest':[0,0,1], \n",
        "    'field':[1,0,0]\n",
        "    }\n",
        "\n",
        "# possible values of f\n",
        "f = np.arange(0, 1, 0.01)\n",
        "\n",
        "# And then plot\n",
        "fig = plt.figure(figsize=(8, 3))\n",
        "axs = fig.gca()\n",
        "for c in contexts:\n",
        "    axs.fill(\n",
        "        f,\n",
        "        contexts[c].pFlower(f),\n",
        "        alpha = 0.3,\n",
        "        color = colors[c],\n",
        "        label = c)\n",
        "axs.legend()\n",
        "axs.set_xlabel('Flower color')\n",
        "axs.set_ylabel('Probability')\n",
        "axs.set_yticks([])\n",
        "axs.set_title('Distribution of flower colors')\n",
        "sns.despine()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeAiHf32Bfmy"
      },
      "source": [
        "# 1. Perceptual decisions\n",
        "\n",
        "Ok, so now we are going to model how given an observation of flower color we can decide which context (*fields* or *forests*) we are in. \n",
        "\n",
        "In part 1 we did something similar, there we asked ~ what is the *true* flower color given the observation of flower color we made. Here, we simplify (or perhaps complicate this) given a flower color we want to classify it into which category it came from. This is more akin to some decision-making tasks we setup in the lab\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So first, using what you've learnt about Bayesian inference, code up a function that returns the likelihood function - that is the probability of observing any given flower color (hence a function of flower color) given that we're in forests or fields.\n",
        "\n",
        "$$ L(f | c = \\text{forest}) = p(f | c = \\text{forest}) $$\n",
        "\n",
        "Here's a schematic of this generative model, with solid bubbles being observables and empty ones being hidden\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"./figures/f1.png\" alt=\"Generative Model\" width=\"100\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YH2AXEM_fQxPZ7tz8bMzOBEXBeuLINpg\" width=\"100\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv5FAt1YVz8f"
      },
      "outputs": [],
      "source": [
        "def lik_function(f, context_name:str, contexts: Dict[str, Context]):\n",
        "    \"\"\"\n",
        "    Function that returns the likelihood ratio of an observed flower color f given forest v.s. field contexts\n",
        "        f: observed flower color(s)\n",
        "        context: class with parameters of flower distributions\n",
        "    Returns:\n",
        "        lik_ratio: ratio of likelihoods of observed flower colors given forest v.s. field contexts\n",
        "    \"\"\"\n",
        "    \n",
        "    # write the expression for computing the likelihood function here\n",
        "    lr = ???\n",
        "    return lr\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this function, we're going to write down a *decision rule* (i.e. a way of deciding which context we're in) that *maximizes* this function. We can express this operation of maximising likelihood in a few different ways:\n",
        "* Picking the context $c$ that has a higher likelihood $L(f | c)$:\n",
        "$$\\text{choose forest if } L(f | c = \\text{forest}) > L(f | c = \\text{field})$$\n",
        "* Picking $\\text{forest}$ if the *likelihood ratio* (LR) of forest to field is >1:\n",
        "$$\\text{choose forest if LR} \\equiv \\frac{L(f | c = \\text{forest})}{L(f | c = \\text{field})}>1$$\n",
        "* Taking the log, and picking $\\text{forest}$ if the *log likelihood ratio* (LLR) is >0:\n",
        "$$\\text{choose forest if LLR}  \\equiv log \\bigg( \\frac{L(f | c = \\text{forest})}{L(f | c = \\text{field}} \\bigg)>0$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us plot the likelihood function you coded up, and also examine the log likelihood ratios of 1000 observed flower colors, 500 from each context!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "qm3M689GWsjf",
        "outputId": "5771fcd3-5eee-4b7b-e148-cda0c4c55d3c"
      },
      "outputs": [],
      "source": [
        "# Convenient method to plot any given function of flower color for both contexts\n",
        "def plot_func_of_f_c(func, func_name: str, f, contexts, colors, **func_kwargs):\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    y = dict()\n",
        "    for c in contexts:\n",
        "        y[c]=func(f,c, contexts, **func_kwargs)\n",
        "        plt.plot(f, y[c], '-', color = colors[c], label = c)\n",
        "    threshold = min(f[y['forest']>y['field']])\n",
        "    plt.vlines(threshold,min([min(y['forest']), min(y['field'])]),max([max(y['forest']), max(y['field'])]),linestyles='dashed',colors='k')\n",
        "    \n",
        "    plt.xlabel('Flower color')\n",
        "    plt.ylabel(f\"{func_name}\")\n",
        "    plt.legend()\n",
        "    plt.title(f\"{func_name} as a function of flower color, context\")\n",
        "    sns.despine()\n",
        "    return threshold\n",
        "\n",
        "threshold = plot_func_of_f_c(lik_function, 'Likelihood', f, contexts, colors)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How do you think we can implement the decision rule of *maximizing* this function? To build intuition, let's simulate some datapoints and examine the log likelihood ratio (LLR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nsamples = 1000\n",
        "\n",
        "loglik_ratios = dict()\n",
        "data = dict()\n",
        "for c in contexts:\n",
        "    data[c] = contexts[c].sample(n = int(nsamples/2))\n",
        "    loglik_ratios[c] = ??? \n",
        "    plt.scatter(data[c], loglik_ratios[c], color = colors[c],alpha=0.1)\n",
        "plt.xlim([0,1])\n",
        "plt.xlabel('Flower color')\n",
        "plt.ylabel('Log likelihood ratio')\n",
        "plt.title('Log likelihood ratios of forest v.s. field')\n",
        "plt.hlines(0,0,1,colors='k')\n",
        "plt.vlines(threshold, min(loglik_ratios['field']), max(loglik_ratios['forest']), linestyles='dashed',colors='k')\n",
        "sns.despine()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_mZa4D0dnOO"
      },
      "source": [
        "### Do you notice anything interesting?\n",
        "\n",
        "> write your answer here (see solutions to check your intuitions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_1WfY9mJe-zX"
      },
      "source": [
        "Let's get Bayesian! We know that the likelihood doesn't tell the entire story, and as Bayesians we must take into account information about the *prior* prevalence of the two categories.\n",
        "\n",
        "Let's code up the *posterior* probability of being in a forest v.s. field given the observations, by combining the likelihood and prior. \n",
        "\n",
        "HINT: Remember that the posterior probability is a *probability* so it must be normalized!\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ \\begin{align}\n",
        "p(c  = \\text{forest}| f) &âˆ p(f | c= \\text{forest}) \\times p(c= \\text{forest}) \\\\\n",
        " &= \\frac{p(f | c= \\text{forest}) \\times p(c= \\text{forest})}{p(f | c= \\text{forest}) \\times p(c= \\text{forest}) + p(f | c= \\text{field}) \\times p(c= \\text{field})}\n",
        " \\end{align}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1VMg_vbfBf2"
      },
      "outputs": [],
      "source": [
        "def posterior_probability(f, chosen_context:str, contexts: Dict[str,Context], priors:Dict[str,float]):\n",
        "    \"\"\"\n",
        "    Function that returns the posterior probability (upto a scaling constant) of an observed frequency f given a context and priors\n",
        "        f: observed flower color(s)\n",
        "        chosen_context: context for which posterior is being evaluated\n",
        "        contexts: dictionary of Context classes\n",
        "        prior: prior probability of that context\n",
        "    Returns:\n",
        "        posterior: posterior probability (upto a scaling constant) of observed flower colors given a context\n",
        "    \"\"\"\n",
        "    # write the expression for computing the posterior here\n",
        "    unchosen_context = [c for c in contexts if c!=chosen_context][0]\n",
        "    posterior_chosen = ???\n",
        "    posterior_unchosen = ???\n",
        "    denom = ???\n",
        "    return posterior_chosen/denom"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the posterior function when the prior probabilities of the two contexts are unequal. What happens to the decision rule then?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "Wie1QxZzfccZ",
        "outputId": "acf6e6ba-6050-4421-ae86-16c7887e998b"
      },
      "outputs": [],
      "source": [
        "priors = {\n",
        "    'forest': 0.9,\n",
        "    'field': 0.1\n",
        "}\n",
        "threshold = plot_func_of_f_c(posterior_probability, 'Posterior probability', f, contexts, colors, priors = priors)\n",
        "plt.vlines(0.5,0,1,linestyles='dotted')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qBNETVrvhUSH"
      },
      "source": [
        "#### What should happens at \"ambiguous\" flower colors of 0.5 when the prior is skewed? Should this affect the decision rule, and if so how?\n",
        "\n",
        "> write answer (See solutions to check your intuitions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hxFaAe4qjAoT"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "Ok, now we are going to move even closer to real-life decisions.\n",
        "\n",
        "For each possible \"ground truth\" context, there are two possible decisions we could have made, one correct and one incorrect. However, these may not be equally good/bad - different errors could have different costs, similarly different correct decisions could have different rewards.\n",
        "\n",
        "Bayesian decision theory tells us we should take these into account for our decision rule, by thresholding the **utility ratio**:\n",
        "\n",
        "$$\n",
        "\\frac{U(\\text{choose forest})}{U(\\text{choose field})} = \\left( \\frac{r_{\\text{forest}}(\\text{choose forest}).P(c = \\text{forest} | f) - c_{\\text{field}}(\\text{choose forest}).P(c = \\text{field} | f)}{r_{\\text{field}}(\\text{choose field}).P(c = \\text{field} | f) - c_{\\text{forest}}(\\text{choose field}).P(c = \\text{forest} | f)} \\right)\n",
        "$$\n",
        "\n",
        "This introduces additional variables: the action we take $a$ (choose forest, choose field), and the reward $r$ / cost $c$ of correct/incorrect decisions\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"./figures/f15.png\" alt=\"Generative Model\" width=\"100\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10BjVANeoLWjSQUUUREIQ8GxsrtJRpfzi\" width=\"100\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "Code this up & play with different costs and rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8joGcfkuq4"
      },
      "outputs": [],
      "source": [
        "def utility_function(f, chosen_context:str, contexts: Dict[str, Context], priors:Dict[str,float], rewards:Dict[str,float], costs:Dict[str,float]):\n",
        "    \"\"\"\n",
        "    Function that returns the utility function of choosing a particular context given an observed frequency f and rewards/costs\n",
        "        f: observed frequency\n",
        "        contexts: context class\n",
        "        prior: ratio of priors for each context\n",
        "        reward: reward for correctly choosing that context\n",
        "        cost: cost for incorrectly choosing that context\n",
        "    Returns:\n",
        "        utility: utility function for choosing that context\n",
        "    \"\"\"\n",
        "    # Compute posteriors for both contexts\n",
        "    unchosen_context = [c for c in contexts if c!=chosen_context][0]\n",
        "    posterior_chosen = ???\n",
        "    posterior_unchosen = ???\n",
        "    \n",
        "    # Compute utility for chosen context\n",
        "    utility = ???\n",
        "    \n",
        "    return utility\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the utility ratios for a number of observed flower color samples, given a prior ratio, reward ratio and associated costs. Play around with these values to see what happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "XmVPGPbqlX6j",
        "outputId": "67a6af43-5b0b-4a0d-e2bb-fb3d3cdf3150"
      },
      "outputs": [],
      "source": [
        "# set the prior to be high for forest and \n",
        "# the cost of incorrectly choosing forest when the context is really field to be really high\n",
        "\n",
        "priors = {\n",
        "    'forest': 0.9,\n",
        "    'field': 0.1\n",
        "}\n",
        "\n",
        "rewards = { # of correctly choosing each context\n",
        "    'forest': 1,\n",
        "    'field': 1\n",
        "}\n",
        "\n",
        "costs = { # of incorrectly choosing each context\n",
        "    'forest': 91,\n",
        "    'field': 11\n",
        "}\n",
        "\n",
        "\n",
        "threshold = plot_func_of_f_c(utility_function, 'Utility function', f, contexts, colors, priors = priors, rewards=rewards, costs=costs)\n",
        "plt.vlines(0.5,-90,0,linestyles='dotted')\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6g5weYbtmF0D"
      },
      "source": [
        "#### What do you notice about the interplay between prior ratios and reward ratios?\n",
        "\n",
        "(see solutions to check your intuitions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "\n",
        "Bayesian Decision Theory (BDT) extends Bayesian inference to *decisions* i.e. discrete choices between (unobservable) categories, based on observed evidence. \n",
        "\n",
        "In BDT, we consider not just the (posterior) probabilities of these categories given the evidence, but also the rewards/costs of making correct/incorrect decisions. \n",
        "These two ingredients combine to form the *utility* of different decisions, and BDT lets us make decisions that maximize this utility.\n",
        "\n",
        "Consequently, BDT is a reminder of the fact that optimal decision-making agents can be biased towards one decision or another for many different reasons - because that decision is more prevalent, or because the associated rewards/costs are high."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_5_avKgWWmf6"
      },
      "source": [
        "# 2. Hierarchical inference\n",
        "\n",
        "So far, we've assumed direct access to the flower colors $f$. However, in reality we may only have access to noisy sensory observations $x$ from a \"color detector\" neuron. This modifies our generative model to a new hierarchical graph:\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"./figures/f2.png\" alt=\"Generative Model\" width=\"100\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Zjt9nhbkAHvQzyiatmEDAjuex9oprduU\" width=\"100\"/>\n",
        "</div>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's assume that the color detector neuron has firing rate centered around the true color, with sensory noise characterized by a variance of $\\sigma_{\\text{sensory}}^2$ The tuning function of such a neuron can be written as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aev477P7WnJm"
      },
      "outputs": [],
      "source": [
        "def tuningFun(f, beePars):\n",
        "    # Gaussian noise on firing rate centered at the color\n",
        "    rate = f + np.random.normal(0, beePars['sigmaSensory'], size=f.shape) \n",
        "    return rate"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yugIvo_6cYpg"
      },
      "source": [
        "In this case, we want to *marginalize* over the frequency, since we still want to infer the variable fo interest c. That is, we are interested in\n",
        "\n",
        "$$\n",
        "P(x \\mid c) = \\int P(x \\mid f) P(f \\mid c) \\, df.\n",
        "$$\n",
        "\n",
        "If we assume that both $P(x \\mid f)$ and $P(f \\mid c)$ are Gaussian with variances $\\sigma_{\\text{sensory}}^2$ and $\\sigma_{f}^2$ respectively, \n",
        "$$P(x \\mid f) = \\mathcal{N}(x | f, \\sigma^2_\\text{sensory})$$\n",
        "$$P(f \\mid c) = \\mathcal{N}(f | c, \\sigma^2_\\text{f})$$\n",
        "\n",
        "then the integral also becomes gaussian - and assuming that the firing rate distribution is centered around the true frequency, this new Gaussian has variance $\\sigma_{f}^2 + \\sigma_{sensory}^2$ \n",
        "\n",
        "This is quite a useful property of gaussians, so we have included a derivation below (Expand the cell)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To derive the marginal probability $P(x \\mid c)$ given that $P(x \\mid f)$ and $P(f \\mid c)$ are Gaussian distributions, let's start with the given integral:\n",
        "\n",
        "$$\n",
        "P(x \\mid c) = \\int P(x \\mid f) P(f \\mid c) \\, df\n",
        "$$\n",
        "\n",
        "Assume:\n",
        "$$\n",
        "P(x \\mid f) = \\mathcal{N}(x \\mid f, \\sigma_{\\text{sensory}}^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\text{sensory}}^2}} \\exp\\left(-\\frac{(x - f)^2}{2\\sigma_{\\text{sensory}}^2}\\right)\n",
        "$$\n",
        "$$\n",
        "P(f \\mid c) = \\mathcal{N}(f \\mid c, \\sigma_{f}^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_{f}^2}} \\exp\\left(-\\frac{(f - c)^2}{2\\sigma_{f}^2}\\right)\n",
        "$$\n",
        "\n",
        "Substituting these Gaussian distributions into the integral, we get:\n",
        "\n",
        "$$\n",
        "P(x \\mid c) = \\int \\frac{1}{\\sqrt{2\\pi\\sigma_{\\text{sensory}}^2}} \\exp\\left(-\\frac{(x - f)^2}{2\\sigma_{\\text{sensory}}^2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_{f}^2}} \\exp\\left(-\\frac{(f - c)^2}{2\\sigma_{f}^2}\\right) \\, df\n",
        "$$\n",
        "\n",
        "Combining the constants outside the integral:\n",
        "\n",
        "$$\n",
        "P(x \\mid c) = \\frac{1}{2\\pi \\sqrt{\\sigma_{\\text{sensory}}^2 \\sigma_{f}^2}} \\int \\exp\\left(-\\frac{(x - f)^2}{2\\sigma_{\\text{sensory}}^2} - \\frac{(f - c)^2}{2\\sigma_{f}^2}\\right) \\, df\n",
        "$$\n",
        "\n",
        "Next, combine the exponents:\n",
        "\n",
        "$$\n",
        "-\\frac{(x - f)^2}{2\\sigma_{\\text{sensory}}^2} - \\frac{(f - c)^2}{2\\sigma_{f}^2}\n",
        "$$\n",
        "\n",
        "To simplify this, let's expand both terms:\n",
        "\n",
        "$$\n",
        "-\\frac{(x - f)^2}{2\\sigma_{\\text{sensory}}^2} = -\\frac{1}{2\\sigma_{\\text{sensory}}^2} (x^2 - 2xf + f^2)\n",
        "$$\n",
        "$$\n",
        "-\\frac{(f - c)^2}{2\\sigma_{f}^2} = -\\frac{1}{2\\sigma_{f}^2} (f^2 - 2fc + c^2)\n",
        "$$\n",
        "\n",
        "Combine the exponents:\n",
        "\n",
        "$$\n",
        "-\\frac{x^2}{2\\sigma_{\\text{sensory}}^2} + \\frac{xf}{\\sigma_{\\text{sensory}}^2} - \\frac{f^2}{2\\sigma_{\\text{sensory}}^2} - \\frac{f^2}{2\\sigma_{f}^2} + \\frac{fc}{\\sigma_{f}^2} - \\frac{c^2}{2\\sigma_{f}^2}\n",
        "$$\n",
        "\n",
        "Group the terms involving $f$:\n",
        "\n",
        "$$\n",
        "= -\\frac{x^2}{2\\sigma_{\\text{sensory}}^2} + \\frac{xf}{\\sigma_{\\text{sensory}}^2} - \\left( \\frac{1}{2\\sigma_{\\text{sensory}}^2} + \\frac{1}{2\\sigma_{f}^2} \\right) f^2 + \\frac{fc}{\\sigma_{f}^2} - \\frac{c^2}{2\\sigma_{f}^2}\n",
        "$$\n",
        "\n",
        "Combine the coefficients of $f$:\n",
        "\n",
        "$$\n",
        "= -\\left( \\frac{1}{2\\sigma_{\\text{sensory}}^2} + \\frac{1}{2\\sigma_{f}^2} \\right) f^2 + \\left( \\frac{x}{\\sigma_{\\text{sensory}}^2} + \\frac{c}{\\sigma_{f}^2} \\right) f - \\left( \\frac{x^2}{2\\sigma_{\\text{sensory}}^2} + \\frac{c^2}{2\\sigma_{f}^2} \\right)\n",
        "$$\n",
        "\n",
        "Complete the square for the $f$-dependent part. Let $A = \\frac{1}{\\sigma_{\\text{sensory}}^2} + \\frac{1}{\\sigma_{f}^2}$, $B = \\frac{x}{\\sigma_{\\text{sensory}}^2} + \\frac{c}{\\sigma_{f}^2}$:\n",
        "\n",
        "$$\n",
        "= -\\frac{A}{2} \\left( f^2 - \\frac{2B}{A} f \\right)\n",
        "$$\n",
        "\n",
        "Add and subtract the square of the middle term inside the parentheses:\n",
        "\n",
        "$$\n",
        "= -\\frac{A}{2} \\left( f^2 - \\frac{2B}{A} f + \\left(\\frac{B}{A}\\right)^2 - \\left(\\frac{B}{A}\\right)^2 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\frac{A}{2} \\left( \\left( f - \\frac{B}{A} \\right)^2 - \\left(\\frac{B}{A}\\right)^2 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\frac{A}{2} \\left( f - \\frac{B}{A} \\right)^2 + \\frac{B^2}{2A}\n",
        "$$\n",
        "\n",
        "Now, the exponent is:\n",
        "\n",
        "$$\n",
        "-\\frac{A}{2} \\left( f - \\frac{B}{A} \\right)^2 - \\left( \\frac{x^2}{2\\sigma_{\\text{sensory}}^2} + \\frac{c^2}{2\\sigma_{f}^2} \\right) + \\frac{B^2}{2A}\n",
        "$$\n",
        "\n",
        "Notice that the integral of the Gaussian function over \\( f \\) (which is a Gaussian integral) results in 1:\n",
        "\n",
        "$$\n",
        "\\int \\exp\\left( -\\frac{A}{2} \\left( f - \\frac{B}{A} \\right)^2 \\right) \\, df = \\sqrt{\\frac{2\\pi}{A}}\n",
        "$$\n",
        "\n",
        "So, we are left with the term outside the integral:\n",
        "\n",
        "$$\n",
        "P(x \\mid c) = \\frac{1}{2\\pi \\sqrt{\\sigma_{\\text{sensory}}^2 \\sigma_{f}^2}} \\sqrt{\\frac{2\\pi}{A}} \\exp\\left( \\frac{B^2}{2A} - \\left( \\frac{x^2}{2\\sigma_{\\text{sensory}}^2} + \\frac{c^2}{2\\sigma_{f}^2} \\right) \\right)\n",
        "$$\n",
        "\n",
        "Given that $A = \\frac{1}{\\sigma_{\\text{sensory}}^2} + \\frac{1}{\\sigma_{f}^2}$ and $B = \\frac{x}{\\sigma_{\\text{sensory}}^2} + \\frac{c}{\\sigma_{f}^2}$, simplify $\\frac{B^2}{2A}$:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{B^2}{2A} = \\frac{\\left( \\frac{x}{\\sigma_{\\text{sensory}}^2} + \\frac{c}{\\sigma_{f}^2} \\right)^2}{2 \\left( \\frac{1}{\\sigma_{\\text{sensory}}^2} + \\frac{1}{\\sigma_{f}^2} \\right)} = \\frac{\\left( \\frac{x}{\\sigma_{\\text{sensory}}^2} + \\frac{c}{\\sigma_{f}^2} \\right)^2 \\sigma_{\\text{sensory}}^2 \\sigma_{f}^2}{2 \\left( \\sigma_{\\text{sensory}}^2 + \\sigma_{f}^2 \\right)}\n",
        "$$\n",
        "\n",
        "Thus, the resulting distribution $P(x \\mid c)$ is Gaussian with mean $c$ and variance $\\sigma_{\\text{sensory}}^2 + \\sigma_{f}^2$:\n",
        "\n",
        "$$\n",
        "P(x \\mid c) = \\frac{1}{\\sqrt{2\\pi (\\sigma_{\\text{sensory}}^2 + \\sigma_{f}^2)}} \\exp\\left(-\\frac{(x - c)^2}{2(\\sigma_{\\text{sensory}}^2 + \\sigma_{f}^2)}\\right)\n",
        "$$\n",
        "\n",
        "Therefore, the new variance is the sum of the original variances because they represent independent sources of noise.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we go on and code the marginal distribution, lets define the parameters of the two contexts and the bee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Parameters of the bee's *internal* distribution of noisy sensory observations\n",
        "beePars = {'sigmaSensory': 0.1}\n",
        "\n",
        "### In contrast, these are the parameters of the *environmental* distribution of flower colors\n",
        "contexts = {\n",
        "    'forest': Context(pFlowerMu = 0.66, pFlowerSig = 0.1), \n",
        "    'field': Context(pFlowerMu = 0.33, pFlowerSig = 0.1) \n",
        "    }\n",
        "\n",
        "# define some useful variables for plotting\n",
        "colors = {\n",
        "    'forest':[0,0,1], \n",
        "    'field':[1,0,0]\n",
        "    }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Complete the function below to compute marginal likelihood $P(x|c)$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDCoCFTCcZJS"
      },
      "outputs": [],
      "source": [
        "def marginal_lik(x, context, beePars):\n",
        "    \n",
        "    marginal_lik = norm.pdf(\n",
        "        x,\n",
        "        loc = ???,\n",
        "        scale = ???\n",
        "        )\n",
        "    \n",
        "    return marginal_lik\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TTfJe7bumoWE"
      },
      "source": [
        "#### Monte carlo techniques to approximate marginalization\n",
        "\n",
        "Here, because each of our sources of noise was Gaussian and independent we could (painstakingly) derive the integral to get to the marginal distribution. There may be many cases where this integral becomes intractable to evaluate analytically - in such cases we can use *simulation* to approximate it - such approaches are called Monte Carlo techniques and have wide applications in real world problems.\n",
        "\n",
        "$$\n",
        "P(x \\mid c) \\approx \\frac{1}{N} \\sum_{i=1}^{N} P(x \\mid f_i), \\quad \\text{where} f_i \\sim P(f \\mid c)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def marginal_lik_sim(x, context, beePars):\n",
        "    f_sample = context.sample(n = 100)\n",
        "    p_sample = np.vstack([norm.pdf(\n",
        "        x,\n",
        "        loc = ???,\n",
        "        scale = ???\n",
        "        ) for f_i in f_sample])\n",
        "    return p_sample.mean(axis = 0)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us plot both the analytical and simulation-based marginal distributions to confirm that they match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "hK_UiPh1Bfmz",
        "outputId": "14532806-f53f-49bc-a2dd-e11763ae0ad1"
      },
      "outputs": [],
      "source": [
        "def compare_analytical_simulated_marginal_likelihoods(contexts, beePars):\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "    for c in contexts:\n",
        "\n",
        "        # Plot observed firing rates for different true flower colors\n",
        "        data = contexts[c].sample(n=100)\n",
        "        rate = tuningFun(data,beePars)\n",
        "        axs[0].plot(data, data, color = 'k')\n",
        "        axs[0].set_ylim([0,1])\n",
        "        axs[0].scatter(data, rate, color = colors[c], alpha = 0.3)\n",
        "        axs[0].set_xlabel('True flower color')\n",
        "        axs[0].set_ylabel('Firing rate $(x)$')\n",
        "        axs[0].set_title('Firing rates $(x)$ observed\\nfor the two contexts')\n",
        "\n",
        "        # we will complute analytical and simulated marginal distributions \n",
        "        # over all these possible values of x (firing rate)\n",
        "        x = np.linspace(rate.min(),rate.max(),100)\n",
        "\n",
        "        # Plot analytical marginal distribution\n",
        "        axs[1].plot(\n",
        "            marginal_lik(x, contexts[c], beePars),\n",
        "            x, \n",
        "            color = colors[c],\n",
        "            alpha= 0.3, \n",
        "            label='analytical')\n",
        "        \n",
        "        # Plot simulated marginal distribution\n",
        "        axs[1].plot(\n",
        "            marginal_lik_sim(x, contexts[c], beePars),\n",
        "            x, \n",
        "            ls = '--',\n",
        "            color = colors[c],\n",
        "            alpha= 0.3, \n",
        "            label = 'monte carlo')\n",
        "\n",
        "        # Plot the distribution of firing rates for the two contexts\n",
        "        label = f\"$P(x | c = {c})$\"\n",
        "        axs[1].hist(\n",
        "            rate,\n",
        "            color = colors[c],\n",
        "            alpha= 0.3,\n",
        "            label = label, \n",
        "            orientation='horizontal', \n",
        "            density=True)\n",
        "        axs[1].set_xlim([0,5])\n",
        "        axs[1].set_xticks([])\n",
        "        axs[1].legend()\n",
        "        axs[1].set_ylabel('Firing rate')\n",
        "        axs[1].set_xlabel('marginal likelihood $P(x|c)$')\n",
        "        axs[1].set_title('Comparing analytical vs simulated\\nmarginal likelihood $P(x|c)$') \n",
        "\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "compare_analytical_simulated_marginal_likelihoods(contexts, beePars)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, here given a noisy firing rate, we can infer its context using the marginal likelihood, marginalizing over all possible flower colors. For high firing rates (or intense flower colors), the marginal likelihood is larger for \"forest\" context - where the true mean was 0.66, and vice versa. To make *decision* about the context, just like before it will be helpful to compute the ratio of the two marginal likelihoods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def marginal_lik_ratio(x, contexts, beePars):\n",
        "    return ???"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zKDP4cLycP0l"
      },
      "source": [
        "Play around with the sensory noise $\\sigma_{\\text{sensory}}$ to see the effect on (1) marginal likelihoods and then (2) marginal likelihood ratios!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1) marginal likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "beePars = {'sigmaSensory': 0.5} # try values: [0.1,0.2,0.4,0.8]\n",
        "\n",
        "compare_analytical_simulated_marginal_likelihoods(contexts, beePars)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(2) marginal likelihood ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "6V2PlwCBjNQh",
        "outputId": "821bc01c-a86e-4ff3-f936-589a09bb249a"
      },
      "outputs": [],
      "source": [
        "# possible firing rates of color detector neuron.\n",
        "x = np.linspace(0,1,100)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "legend_label = \"$\\sigma_{sensory}$\"\n",
        "\n",
        "# vary the sensory noise of the bee\n",
        "for sigmaSensory in [0.1,0.2,0.4,0.8,0.9]:\n",
        "    \n",
        "    beePars = {'sigmaSensory': sigmaSensory}\n",
        "    plt.plot(\n",
        "        x, \n",
        "        np.log(marginal_lik_ratio(x, contexts, beePars)), \n",
        "        label = f\"{legend_label}={sigmaSensory}\"\n",
        "        )\n",
        "plt.xlabel('Firing rate $(x)$')\n",
        "plt.ylabel('Marginal log likelihood ratio')\n",
        "plt.title(\"Effect of sensory noise on marginal LLR\")\n",
        "sns.despine()\n",
        "plt.legend(bbox_to_anchor=[1.1, 0.8])\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Think about\n",
        "- What happens to marginal likelihoods and marginal likelihood ratios? \n",
        "- Where should the decision boundary be drawn? \n",
        "- Is the bee still able to *correctly* decide which context it might be in?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">  [WRITE ANSWER]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5dDYBNCDBfm1"
      },
      "source": [
        "## Inferring changing contexts\n",
        "Ok, now we will let the bee fly around. So the context it is in might change. We will model the decision dyanmics of the bee in such changing contexts. \n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"./figures/f3.png\" alt=\"Generative Model\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12O54pIopjwTWtJoOu5yohjcQ4XudIAW7\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**BUT** before going there, we will learn how to do online Bayesian inference (also called recursive Bayes or Bayesian filtering) in a more restricted setting - one in which the contexts change without the bee having any control over whether it samples from field or forest. This is almost like the bee is trapped in a car and the car passes through patches of fields and forests."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First, we will define a class that lets us simulate a bee being driven around through patches of forests and fields.\n",
        "In addition to the parameters we have had for the two contexts (namely $\\mu_c$) and bee parameters (sensory noise $\\sigma_{\\text{sensory}}^2$), we are going to introduce another parameter $p_{switch}$ which is the probability of the context changing. We are going to assume that this change process is Markovian i.e. there is a fixed probability of change occuring at every time step.\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"./figures/f4.png\" alt=\"Generative Model\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fRKUIA96_P0sqq3ItwsdCTz2fnsC2IG_\" width=\"300\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiIJH706Bfm1"
      },
      "outputs": [],
      "source": [
        "class simulateEnv:\n",
        "    \n",
        "    def __init__(self, contexts, beePars, **envParams):\n",
        "        self.contexts = contexts\n",
        "        self.beePars = beePars\n",
        "        self.switchType = envParams.get('switchType', None)\n",
        "        self.nSwitch = envParams.get('nSwitch', None)\n",
        "        self.contextLabels = list(self.contexts)\n",
        "\n",
        "\n",
        "    #Sample contexts, observed flower colours aka firing rates\n",
        "    def sample(self, tMax):\n",
        "        \n",
        "        fObs = np.zeros(tMax)  # observations made by the bee i.e. firing rates of the color neuron\n",
        "        pSwitch = 1/self.nSwitch # Hazard rate i.e. 1/(avg no. encounters before switching contexts)\n",
        "        contextFlag = np.zeros(tMax, dtype='int') # store which context the bee is in (environmental dynamics)\n",
        "\n",
        "        # Simulate observations of flower colors for this assumed rate of context change\n",
        "        \n",
        "        # sample the initial context \n",
        "        contextFlag[0] = np.random.randint(len(self.contextLabels))\n",
        "        context_0 = self.contextLabels[contextFlag[0]]\n",
        "        \n",
        "        # make a firing rate observation in this context (remember marginal likelihood is P(x|c))\n",
        "        fObs[0] = self.sample_from_marginal_lik(context_0, 1)\n",
        "\n",
        "        switch_indicator = False\n",
        "    \n",
        "        # simulate over time\n",
        "        for i in range(1, tMax):\n",
        "            \n",
        "            # switch every nSwitch trials\n",
        "            if self.switchType == 'periodic':\n",
        "                switch_indicator = np.mod(i, self.nSwitch) == 0\n",
        "                \n",
        "            # Switch contexts with a probability of 'pSwitch'\n",
        "            elif self.switchType == 'markov':\n",
        "                switch_indicator = np.random.rand() < pSwitch\n",
        "            else:\n",
        "                raiseException(\"unknow switchType, can be 'periodic' or 'markov\")\n",
        "            \n",
        "            # switch contexts\n",
        "            if switch_indicator:\n",
        "                contextFlag[i] = not(contextFlag[i-1])  \n",
        "            else:\n",
        "                contextFlag[i] = contextFlag[i-1]\n",
        "                \n",
        "            # Sample flower colors from chosen context\n",
        "            fObs[i] = self.sample_from_marginal_lik(self.contextLabels[contextFlag[i]], 1)\n",
        "            \n",
        "            \n",
        "        return contextFlag, fObs\n",
        "    \n",
        "    \n",
        "    # method to sample observations (firing rates) given context\n",
        "    def sample_from_marginal_lik(self, context, n):\n",
        "        \n",
        "        sample = norm.rvs(\n",
        "            loc = self.contexts[context].pFlowerMu,\n",
        "            scale = np.sqrt(self.contexts[context].pFlowerSig**2 + self.beePars['sigmaSensory']**2),\n",
        "            size = n\n",
        "        )\n",
        "    \n",
        "        return sample    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "# plotting function for visualizing firing rates when the contexts are changing\n",
        "def plot_context_and_observations(tMax, contextFlag, fObs, title):\n",
        "\n",
        "    # Plot true context and sensory observations with two y-axes\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 3))\n",
        "\n",
        "    # First subplot (True context)\n",
        "    scatter1 = ax1.scatter(range(tMax), contextFlag, 5, color='k', label='True context')\n",
        "    ax1.set_yticks([0, 1])\n",
        "    ax1.set_yticklabels(list(contexts))\n",
        "    ax1.set_xlabel('Time')\n",
        "\n",
        "    # Second y-axis for the second subplot (Sensory observations)\n",
        "    ax2 = ax1.twinx()\n",
        "    this_color = [0.5, 0.5, 0]\n",
        "    scatter2 = ax2.scatter(range(tMax), fObs, 8, color=this_color, label='Firing rate observations')\n",
        "    ax2.set_ylabel('Firing rate')\n",
        "    ax2.spines['right'].set_color(this_color)\n",
        "    ax2.tick_params(axis='y', colors = this_color)\n",
        "    ax2.yaxis.label.set_color(this_color)\n",
        "\n",
        "    lines = [scatter1, scatter2]\n",
        "    labels = [line.get_label() for line in lines]\n",
        "    ax1.legend(\n",
        "        lines, \n",
        "        labels, \n",
        "        bbox_to_anchor = (0.8, -0.4),\n",
        "        ncol = 2\n",
        "        )\n",
        "    \n",
        "    ax1.set_title(f\"{title} environment\")\n",
        "    sns.despine(right = False)\n",
        "    \n",
        "    return fig, ax1\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a7MxCnujBfm2"
      },
      "source": [
        "Okay, lets's visualize this change: you can change the `switchType` to `periodic` in the cell below, to very clearly visualize distributional shifts in observations due to changing contexts. Also try increasing the sensory noise to see how that changes the observations that the bee makes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "WXRpJuroBfm2",
        "outputId": "4b15a51a-15e0-43aa-b509-3d444840ff1c"
      },
      "outputs": [],
      "source": [
        "## Create a bee with a chosen sensory noise parameter - PLAY WITH THIS!\n",
        "beePars = {'sigmaSensory':0.1}\n",
        "\n",
        "### Parameters of the *environmental* distribution of flower colors\n",
        "contexts = OrderedDict({\n",
        "    'forest': Context(pFlowerMu = 0.66, pFlowerSig = 0.1), \n",
        "    'field': Context(pFlowerMu = 0.33, pFlowerSig = 0.1) \n",
        "    })\n",
        "\n",
        "### number of time points\n",
        "tMax = 200\n",
        "\n",
        "### Simulate context switches, with an average change every 20 trials\n",
        "# Change switchType to 'periodic' to easily understand the relationship between context switch and firing rates\n",
        "envPars = {\n",
        "    'switchType': 'markov',\n",
        "    'nSwitch': 20 \n",
        "    }\n",
        "\n",
        "# initialize class\n",
        "sim = simulateEnv(contexts, beePars, **envPars)\n",
        "\n",
        "### Sample contexts, firing rate samples from that context\n",
        "contextFlag,fObs = sim.sample(tMax)\n",
        "fig, ax = plot_context_and_observations(tMax, contextFlag, fObs, envPars['switchType'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference \n",
        "\n",
        "Now let's write a function to perform online inference as the bee encounters flowers in different contexts. \n",
        "\n",
        "Some terminology first:\n",
        "The posterior probability of the bee thinking that it is in context \"field\", given the sequence of flowers it has encountered upto and including time t is often referred to as the belief: $p_{\\text{belief}, t}^{field}$. \n",
        "\n",
        "This belief will need to be updated at each time step, based on the dynamics of context switches and the encountered flower colors. \n",
        "\n",
        "Now, when the bee encounters its first flower, two steps need to occur:\n",
        "1. Prediction: probability that the bee is still in context \"field\", given that it encountered a flower (irrespective of its color)\n",
        "\n",
        "2. Update: probability that the bee is still in context \"field\", given that it observed the specific color of the flower. \n",
        "\n",
        "Lets, write down what these steps look like in math:\n",
        "\n",
        "##### **the prediction step**:\n",
        "$$p_{\\text{predict}, t = 1}^{\\text{field}} = p^{\\text{field}}_{\\text{belief},t=0}(1-p_{\\text{switch}}) + (1 - p^{\\text{field}}_{\\text{belief},t=0})p_{\\text{switch}} $$\n",
        "This first term here evaluates the probability of being in context field if a switch didn't happen. And the second term computes the probability of being in context field if a switch happened.\n",
        "\n",
        "##### **the update step**:\n",
        "our prediction acts as the prior and we compute the likelihood of observing firing rate $x_1$ given we are in context \"field\" to arrive at the posterior proability of thinking that we are in context \"field\" \n",
        "$$p_{\\text{belief}, t = 1}^{field} \\propto p_{\\text{predict}, t = 1}^{\\text{field}} \\mathcal{N}(x_1; \\sigma^2_{\\text{sensory}}, \\mu_{\\text{field}}, \\sigma^2_{\\text{field}}) $$\n",
        "\n",
        "Again, this process will be repeated at every time step. Therefore, essentially in Bayesian filtering **\"todayâ€™s posterior acts as tomorrowâ€™s priorâ€** (Lindley, Bayesian statistics, a review. 1972, p. 2). This is comparable to evidence accumulation frameworks, such as the DDM, where the observer is thought to accumulate noisy samples of evidence over time. Here, the evidence is the log-likelihood, and accumulation is the Bayes-optimal solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform online inference\n",
        "def onlineInference(fObs, contexts, beePars, pSwitch = 0.5, p0 = [0.5,0.5]):\n",
        "\n",
        "    # Construct transition matrix\n",
        "    T = [[1-pSwitch,pSwitch],[pSwitch,1-pSwitch]]\n",
        "\n",
        "    # Initialize beliefs\n",
        "    pContext = np.zeros([2,np.size(fObs)])\n",
        "    pContext[:,0] = p0\n",
        "    contextLabel = list(contexts)\n",
        "\n",
        "    for t in np.arange(1, np.size(fObs)):\n",
        "        \n",
        "        #Prediction step: apply transition matrix\n",
        "        pPredict = ???\n",
        "\n",
        "        # Update step: Likelihood of observation fObs(t) under the two contexts\n",
        "        lik = [\n",
        "            ???,\n",
        "            ???\n",
        "            ]\n",
        "        \n",
        "        # Update step: Posterior belief \n",
        "        pContext[:,t] = ???\n",
        "        \n",
        "        # normalize\n",
        "        pContext[:,t] = pContext[:,t]/sum(pContext[:,t])\n",
        "\n",
        "    return pContext\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets plot the running posterior belief as well as the posterior belief averaged over switches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a bee with a chosen sensory noise parameter - PLAY WITH THIS!\n",
        "beePars = {'sigmaSensory': 0.1}\n",
        "\n",
        "### Parameters of the *environmental* distribution of flower colors\n",
        "contexts = OrderedDict({\n",
        "    'forest': Context(pFlowerMu = 0.66, pFlowerSig = 0.1), \n",
        "    'field': Context(pFlowerMu = 0.33, pFlowerSig = 0.1) \n",
        "    })\n",
        "\n",
        "# initialize class\n",
        "sim = simulateEnv(contexts, beePars, **envPars)\n",
        "\n",
        "# Sample contexts, firing rate samples from that context\n",
        "contextFlag,fObs = sim.sample(tMax)\n",
        "\n",
        "# infer posterior beliefs\n",
        "pContext = onlineInference(\n",
        "    fObs, \n",
        "    contexts, \n",
        "    beePars, \n",
        "    pSwitch = 1/envPars['nSwitch'],\n",
        "    # pSwitch = 0.5,\n",
        "    p0 = [0.5, 0.5]  # initial beliefs about the two contexts\n",
        "    )\n",
        "\n",
        "# plot the beliefs\n",
        "fig, axs = plot_context_and_observations(tMax, contextFlag, fObs, envPars['switchType'])\n",
        "label = f'$p^{list(contexts)[1]}$'\n",
        "axs.plot(pContext[1,:],color = 'r', label = label)\n",
        "axs.legend( bbox_to_anchor = (1.4, 0.7),)\n",
        "plt.show()\n",
        "\n",
        "# Plot posterior belief averaged over switches\n",
        "pattern = {'Forest->Field':'1111100000', 'Field->Forest':'0000011111'}\n",
        "fig2 = plt.figure(figsize=(16.5, 3))\n",
        "k = 1\n",
        "for s in pattern.keys():\n",
        "    fig2.add_subplot(1, 2 ,k, title = 'Average p('+s+')')\n",
        "    k+=1\n",
        "    pAvg = np.zeros(10)\n",
        "    inds = [m.start() for m in re.finditer(pattern[s], ''.join(str(e) for e in contextFlag))]\n",
        "    for i in inds:\n",
        "        pAvg = pAvg + pContext[1,i:i+10]\n",
        "    pAvg = pAvg/np.size(inds)\n",
        "    plt.plot(np.arange(-4,6), (1 - pAvg), c = 'r')\n",
        "    plt.ylabel('p (field)')\n",
        "    plt.plot([0,0],[0,1],'k--')\n",
        "sns.despine()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### things to think about:\n",
        "1. What happens if you increase the sensory noise, observation noise?\n",
        "2. What happens if the bee think $p_{\\text{switch}}$ is higher than it is?\n",
        "3. What happens if the bee's knowledge of the distribution of flower colors in the two context is not accurate? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> log your observations here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Selecting actions\n",
        "Let's turn now to simulating actions of the bee in this environment! So far, we assumed (perhaps unnaturally) that the bee's context was shifting because it was being driven around. In reality however, the bee can move around - hence changing the context it is in *as a consequence of its actions*! Such a setup takes us from a hidden Markov model (in which hidden states change on their own) to a **Markov decision process* or MDP, in which our decision don't just yield rewards/costs, but can change the state of the world around us.\n",
        "\n",
        "MDPs are the foundation of RL, as you might have already heard. \n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"./figures/f6.png\" alt=\"Generative Model\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1hhMFTuRYNu5Yh8GogIwuYEsgNg9e0CHU\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Let us start by expanding the bee's environment into a 2d patch that it can forage in, which has flowers of different colors signalling different contexts (forests or fields). Additionally, we will also define the rewards available from different flowers, as a function of context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Patch:\n",
        "    # Create 2d flower color pattern\n",
        "    def __init__(self):\n",
        "        X,Y = np.meshgrid(np.linspace(-3,3,100),np.linspace(-3,3,100))\n",
        "        F = -(X**5 - Y**5 - 2 * np.matmul(X, Y) - 2 * X**3 - 3 * Y**3) * (\n",
        "            0.9 * np.exp(-0.4 * (X - 1.5)**2 - 0.5 * (Y - 1.5)**2) -\n",
        "            0.9 * np.exp(-(X + 2)**2 - (Y + 2)**2 - 0.8 * (X + 2) * (Y + 2)) -\n",
        "            0.7 * np.exp(-(X + 2)**2 - 0.5 * (Y - 2)**2 + 0.3 * (X + 2) * (Y - 2)) -\n",
        "            0.5 * np.exp(-3 * (X - 2)**6 - 4 * (Y + 2)**2 + 4 * (X - 2) * (Y + 2)**2))     \n",
        "        F = 1-(F- np.min(F))/(np.max(F)-np.min(F))\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.F = F\n",
        "\n",
        "    # Flower color as a function of location on patch\n",
        "    def f(self,xCurr,yCurr,xNext,yNext):\n",
        "        iCurr = np.max(np.where(self.X<xCurr)[1])\n",
        "        jCurr = np.max(np.where(self.Y<yCurr)[0])\n",
        "        iNext = np.max(np.where(self.X<xNext)[1])\n",
        "        jNext = np.max(np.where(self.Y<yNext)[0])\n",
        "        return self.F[iCurr,jCurr], self.F[iNext,jNext]\n",
        "\n",
        "    # Nectar as a function of flower color in \"field\" or \"forest\" environments\n",
        "    def nectar(self,f,context):\n",
        "        if context =='field':\n",
        "            return 1 - norm.cdf(f, loc = 0.5,scale = 0.15)\n",
        "        elif context =='forest':\n",
        "            return norm.cdf(f, loc = 0.5,scale = 0.15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at this patch, and the available nectar in different contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create a patch for the bee to forage in\n",
        "patch = Patch()\n",
        "\n",
        "# Plot flower frequency patterns in patch\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(121,title = 'Foraging patch')\n",
        "plt.contourf(patch.F,levels=10,cmap=plt.cm.get_cmap('Spectral'))\n",
        "plt.colorbar(orientation = 'horizontal', label = 'Flower color')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot nectar function in the two contexts\n",
        "f = np.arange(0,1,0.01)\n",
        "colors = {'forest':[0,0,1],'field':[1,0,0]}\n",
        "\n",
        "plt.subplot(122,title = 'Nectar function')\n",
        "for context in {'forest','field'}:\n",
        "    plt.plot(f,patch.nectar(f,context),color = colors[context],label = context)\n",
        "plt.xlabel('Flower color')\n",
        "plt.ylabel('Nectar')\n",
        "sns.despine()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now define a bee with the following functions:\n",
        "\n",
        "1. An **inference function** that lets it infer which context it is in, based on the flower colors it observes. To keep it simple, lets assume the bee directly observes flower color and thresholds it directly to decide which context it is in (as in section 1).\n",
        "\n",
        "2. A **value function** - this is similar to the reward/utility functions we have seen before, and this tells the bee how *valuable* different flowers are i.e. how much nectar reward it can expect from different flower colors in different contexts.\n",
        "\n",
        "3. An **action policy** - this is the function that determines how the bee will act, consequently changing the context it is in. We are going to use a very simple policy, where the bee is going to keep going straight if utility/value increases ahead of it, and reorient randomly if it decreases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bee:\n",
        "    def __init__(self,params):\n",
        "        self.params = params\n",
        "        \n",
        "    def inferenceFunc(self,f):\n",
        "        return 'forest' if f>self.params['sensoryThreshold'] else 'field'\n",
        "\n",
        "    #Linear approximation to utility/value function \n",
        "    def valueFunc(self,f,context):\n",
        "        if np.size(f)>0:\n",
        "            return self.params['w'][context][1]*f+self.params['w'][context][0]\n",
        "        else:\n",
        "            return -np.inf #Boundary condition\n",
        "\n",
        "    #Stochastic action policy\n",
        "    def policyFunc(self,vTminus1,vT):\n",
        "        \n",
        "        # Softmax decision to reorient\n",
        "        if np.random.rand()<1/(1+np.exp(self.params['softmaxTemp']*(vT-vTminus1-self.params['softmaxThreshold']))):\n",
        "\n",
        "            # Von-mises distributed reorienting angles\n",
        "            deltaTheta =  np.random.vonmises(self.params['reorientMu'],self.params['reorientKappa'])\n",
        "            \n",
        "            # Turn clockwise (cw), counter-clockwise (ccw), or (both) as determined by 'policyFuncType'\n",
        "            action = {'cw': deltaTheta, 'ccw': -deltaTheta, 'both': np.random.choice([deltaTheta,-deltaTheta])}\n",
        "            \n",
        "            return action[self.params['policyFuncType']]\n",
        "        else:\n",
        "            # Continue straight if not reorienting\n",
        "            return 0\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy with known values\n",
        "Let's instantiate a bee with custom-defined action policy parameters, with a fixed (known) value function. Note that we are approximating the *true* nectar function with a linear value function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create a bee with known, context dependent value function weights 'w'\n",
        "bee = Bee(\n",
        "    params = {\n",
        "        'sensoryThreshold': 0.5,\n",
        "        'policyFuncType': 'cw',\n",
        "        'softmaxTemp':1,\n",
        "        'softmaxThreshold':0,\n",
        "        'reorientMu':np.pi/2, \n",
        "        'reorientKappa':10,\n",
        "        'stepSize':0.5,\n",
        "        'discountGamma':0,\n",
        "        'learningRate':0,\n",
        "        'w':{'forest': np.array([0,1]), 'field':np.array([1,-1])}\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "# Plot policy functions - reorienting probability & angles\n",
        "fig1 = plt.figure(figsize=(10, 3))\n",
        "plt.subplot(121,title = 'Policy: Reorienting probability')\n",
        "plt.plot(\n",
        "    np.linspace(-1,1,100),\n",
        "    1/(1+np.exp(bee.params['softmaxTemp']*(np.linspace(-1,1,100)-bee.params['softmaxThreshold']))))\n",
        "plt.xlabel('Change in value')\n",
        "plt.ylabel('p (Reorient)')\n",
        "\n",
        "plt.subplot(122,title = 'Policy: Reorienting angles')\n",
        "plt.fill(\n",
        "    np.linspace(0,np.pi,100), \n",
        "    vonmises.pdf(np.linspace(0,np.pi,100),bee.params['reorientKappa'],loc = bee.params['reorientMu']),\n",
        "    alpha = 0.4)\n",
        "plt.xticks([0,np.pi/2,np.pi],[0,90,180])\n",
        "plt.xlabel('Change in orientation')\n",
        "sns.despine()\n",
        "\n",
        "# Plot value functions (assumed known)\n",
        "fig2 = plt.figure(figsize=(10, 3))\n",
        "plt.subplot(121,title = 'Value in field')\n",
        "plt.plot(f,bee.valueFunc(f,'field'),color = 'r')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Flower color')\n",
        "plt.subplot(122,title = 'Value in forest')\n",
        "plt.plot(f,bee.valueFunc(f,'forest'),color = 'b')\n",
        "plt.xlabel('Flower color')\n",
        "sns.despine()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's simulate the behavior of the bee on the patch for a single trial with 50 timesteps!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimulateBee:\n",
        "    def __init__(self, bee, patch, simPars):\n",
        "        self.bee = bee\n",
        "        self.patch = patch\n",
        "        self.simPars = simPars\n",
        "        self.f = np.arange(0, 1, 0.01)  \n",
        "        self.fig, (self.axp, self.axv) = plt.subplots(1, 2,figsize=(12,5))\n",
        "        \n",
        "        # Initial position & orientation\n",
        "        self.xCurr, self.yCurr = 0, 0\n",
        "        self.theta = simPars['thetaInit']\n",
        "        self.xNext, self.yNext = self.next_position(self.xCurr, self.yCurr, self.theta)\n",
        "        self.evaluate_position()\n",
        "        \n",
        "    def next_position(self, xCurr, yCurr, theta):\n",
        "        # Next position based on current position & orientation\n",
        "        return xCurr + self.bee.params['stepSize'] * np.cos(theta), yCurr + self.bee.params['stepSize'] * np.sin(theta)\n",
        "        \n",
        "    def evaluate_position(self):\n",
        "        # Sensory observations\n",
        "        self.fCurr, self.fNext = self.patch.f(self.xCurr, self.yCurr, self.xNext, self.yNext)\n",
        "        # Context inference\n",
        "        self.context = self.bee.inferenceFunc(self.fCurr)\n",
        "        # Evalutation of sensory observations given context\n",
        "        self.vCurr, self.vNext = self.bee.valueFunc(self.fCurr, self.context), self.bee.valueFunc(self.fNext, self.context)\n",
        "    \n",
        "    def plan_and_select_action(self, xPrev, yPrev):\n",
        "        # Planning and action selection \n",
        "        plan = True\n",
        "        while plan:\n",
        "            # Prospective position, orientation\n",
        "            deltaTheta = self.bee.policyFunc(self.vCurr, self.vNext)\n",
        "            self.theta += deltaTheta\n",
        "            self.xCurr, self.yCurr = self.next_position(xPrev, yPrev, self.theta)\n",
        "            self.xNext, self.yNext = self.next_position(self.xCurr, self.yCurr, self.theta)\n",
        "\n",
        "            # Check if the new position is within the bounds of the patch\n",
        "            if (self.xNext > np.min(self.patch.X) and self.xNext < np.max(self.patch.X) and\n",
        "                self.yNext > np.min(self.patch.Y) and self.yNext < np.max(self.patch.Y)):\n",
        "                self.evaluate_position()\n",
        "                plan = False\n",
        "            else:\n",
        "                self.vNext = -np.inf\n",
        "        \n",
        "    \n",
        "    def update(self, t):\n",
        "        \n",
        "        # Plot value function\n",
        "        self.axv.clear()\n",
        "        current_context = self.context\n",
        "        other_context = 'forest' if current_context == 'field' else 'field'\n",
        "        alpha = {\n",
        "            current_context:1,\n",
        "            other_context:0.1\n",
        "        }\n",
        "        for context in ['forest','field']:\n",
        "            self.axv.plot(\n",
        "                self.f, \n",
        "                self.bee.valueFunc(self.f, context), \n",
        "                color=colors[context], \n",
        "                label = context,\n",
        "                alpha = alpha[context])\n",
        "        self.axv.set_title('Value in ' + self.context)\n",
        "        self.axv.set_xlabel('Flower color')\n",
        "        self.axv.set_ylabel('Value') \n",
        "        self.axv.set_ylim(0, 3)\n",
        "        self.axv.legend()\n",
        "        sns.despine(ax = self.axv)\n",
        "\n",
        "        # Plot patch & bee position\n",
        "        self.axp.clear()\n",
        "        self.axp.axis('off')\n",
        "        self.fig.set_facecolor('white')\n",
        "        self.axp.contourf(self.patch.X, self.patch.Y, self.patch.F, levels=10, cmap='Spectral')\n",
        "        self.axp.set_title(self.context)\n",
        "\n",
        "        # Planning and action selection\n",
        "        xPrev, yPrev = self.xCurr, self.yCurr\n",
        "        self.plan_and_select_action(xPrev, yPrev)\n",
        "\n",
        "        # Plot movement to new location\n",
        "        self.axp.quiver(xPrev, yPrev, self.xCurr - xPrev, self.yCurr - yPrev, scale=7, width=0.003)\n",
        "\n",
        "    def animate(self):\n",
        "        anim = FuncAnimation(self.fig, self.update, frames=self.simPars['tMax'], interval=200)\n",
        "        return anim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run simulation. \n",
        "simPars = {\n",
        "    'tMax': 100, \n",
        "    'thetaInit': np.random.vonmises(0, 0.1)\n",
        "    }\n",
        "contexts = ['field', 'forest']\n",
        "\n",
        "%matplotlib notebook\n",
        "sim = SimulateBee(bee, patch, simPars)\n",
        "anim = sim.animate()\n",
        "html_content = anim.to_jshtml()\n",
        "display(HTML(f\"{html_content}\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What do you notice about the bee's inferred context & value function?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> log your observations here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning values with reinforcement learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now turn to the final challenge - what happens if you *don't* know the value function, but have to learn it from experience with rewards in the environment? This is the problem of reinforcement learning. Thing get tricky *very fast* when we mix uncertainty about sensory context and uncertainty about value - so we will use a popular assumption that you can treat them in sequence - first do contextaul inferences and then learn value conditional on the inferred context.\n",
        "\n",
        "Here, we are using a popular RL technique called temporal difference learning, in order to learn *state values* that is values as a function of the sensory state - in this case flower color. We are also going to be using function approximation, to approximate the true value with a linear function. Play around with the policy - specifically the temperature, which controls exploration. You will see how easy it is to get stuck in a situation where you are unable to learn any further!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BeeWithLearning(Bee):\n",
        "    \n",
        "    def learningFunc(self, f, context, vTminus1, vT, reward):\n",
        "        #Temporally discounted reward prediction error\n",
        "        delta = reward + self.params['discountGamma']*vT - vTminus1\n",
        "        # Gradient of value w.r.t. weights\n",
        "        gradfun = np.array([1, f])\n",
        "        # Weight update using Temporal Difference learning rule\n",
        "        self.params['w'][context] = self.params['w'][context] + delta*self.params['learningRate']*gradfun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimulateBeeWithLearning(SimulateBee):\n",
        "\n",
        "    def update(self, t):\n",
        "\n",
        "        # Plot current learned estimate of value function\n",
        "        self.axv.clear()\n",
        "        current_context = self.context\n",
        "        other_context = 'forest' if current_context == 'field' else 'field'\n",
        "        alpha = {\n",
        "            current_context:1,\n",
        "            other_context:0.1\n",
        "        }\n",
        "        for context in ['forest','field']:\n",
        "            self.axv.plot(\n",
        "                self.f, \n",
        "                self.bee.valueFunc(self.f, context),\n",
        "                color=colors[context], \n",
        "                label = context,\n",
        "                alpha = alpha[context]\n",
        "                )\n",
        "        self.axv.set_title('Value in ' + self.context)\n",
        "        self.axv.set_xlabel('Flower color')\n",
        "        self.axv.set_ylabel('Value')\n",
        "        self.axv.set_ylim(0, 3)\n",
        "        self.axv.legend()\n",
        "        sns.despine(ax = self.axv)\n",
        "\n",
        "        # Plot patch environment\n",
        "        self.axp.clear()\n",
        "        self.axp.contourf(self.patch.X, self.patch.Y, self.patch.F, levels=10, cmap='Spectral')\n",
        "        self.axp.set_title(self.context)\n",
        "\n",
        "        # Planning and action selection\n",
        "        xPrev, yPrev = self.xCurr, self.yCurr\n",
        "        vPrev = self.bee.valueFunc(self.fCurr, self.context)\n",
        "        self.plan_and_select_action(xPrev, yPrev)\n",
        "\n",
        "        # ~~~~~~~~~~~~~~~~~~~~~ LEARNING RULE~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "        # Observe reward outcome of action, update weights based on learning rule\n",
        "        reward = self.patch.nectar(self.fCurr, self.context)\n",
        "        self.bee.learningFunc(self.fCurr, self.context, vPrev, self.vCurr, reward)\n",
        "\n",
        "        # Plot movement to new location\n",
        "        self.axp.quiver(xPrev, yPrev, self.xCurr - xPrev, self.yCurr - yPrev, scale=7, width=0.003)\n",
        "        sns.despine(ax = self.axp)\n",
        "        self.fig.set_facecolor(\"white\")\n",
        "        self.fig.tight_layout()\n",
        "        \n",
        "\n",
        "    def animate(self):\n",
        "        anim = FuncAnimation(self.fig, self.update, frames=self.simPars['tMax'], interval=200)\n",
        "        plt.show()\n",
        "        return anim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming createBee, createPatch, and other necessary classes and functions are defined elsewhere\n",
        "# TRY CHANGING THE SOFTMAX TEMP! WHEN VERY HIGH, THE BEE WON'T EXPLORE THE OTHER CONTEXT- HENCE WON'T LEARN ABOUT IT\n",
        "bee2 = BeeWithLearning(\n",
        "    params = {\n",
        "        'sensoryThreshold':0.5,\n",
        "        'policyFuncType': 'both',\n",
        "        'softmaxTemp': 1, \n",
        "        'softmaxThreshold': 0,\n",
        "        'reorientMu': np.pi/2, \n",
        "        'reorientKappa': 10, \n",
        "        'stepSize': 0.5,\n",
        "        'discountGamma': 0.6, \n",
        "        'learningRate': 0.5,\n",
        "        'w': {'forest': np.array([0, 0]), 'field': np.array([0, 0])}\n",
        "        }\n",
        "    )\n",
        "\n",
        "patch = Patch()  \n",
        "### Simulate 200 timesteps\n",
        "simPars = {'tMax': 200, 'thetaInit': np.random.vonmises(0, 0.1)}\n",
        "simulator = SimulateBeeWithLearning(bee2, patch, simPars)\n",
        "anim = simulator.animate()\n",
        "html_content = anim.to_jshtml()\n",
        "display(HTML(f\"{html_content}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HtNv_wLjJPFt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
