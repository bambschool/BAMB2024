{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0063b4",
   "metadata": {},
   "source": [
    "### Part 2: Fitting RL models\n",
    "In this tutorial we will learn how implement agents that make decisions that are based on their previous experience.\n",
    "\n",
    "The code is related to:\n",
    "\n",
    "Bob Wilson & Anne Collins (2019) eLife: Ten simple rules for the computational modeling of behavioral data.\n",
    "\n",
    "In this paper, the authors describe a 2-armed bandit, where a player performs $T$ choices between two options. The machine has asymmetric reward probabilities $\\mu_{1} = 0.2$ and $\\mu_{2} = 0.8$ associated with each arm, which are initially unknown to the player.\n",
    "\n",
    "We want to find out: Which strategy does a subject use to maximize their overall reward on this machine?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6f921",
   "metadata": {},
   "source": [
    "Let's start by importing some modules and making sure the `models.py` module with the helper functions is properly imported if you're working on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Download the custom library - do this manually if not running on google colab\n",
    "MODULE_NAME = \"models.py\"\n",
    "if not os.path.isfile(\"models.py\"):\n",
    "    MODULE_URL = f'https://raw.githubusercontent.com/bambschool/BAMB2024/main/day2_reinforcement_learning/part2_fitting_rl_models/{MODULE_NAME}'\n",
    "    !wget -O {MODULE_NAME} \"{MODULE_URL}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set experimental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num_trials = 100\n",
    "reward_probabilities = np.array([0.2, 0.8])\n",
    "num_repetitions = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate our candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider three different normative models (M2, M3, and M4) of how subjects might attack the problem of maximizing reward on a 2-armed bandit. You can read the mathematical formulation of each model in the paper. Shortly, the proposed models are:\n",
    "\n",
    "- **Noisy win-stay-lose-shift**: Rewarded actions are repeated, unrewarded actions lead to the player choosing a different arm in the next trial. Every now and then, the subject explores a different option (model 2 with 1 parameter, $\\epsilon$, describing the overall level of randomness)\n",
    "\n",
    "- **Rescorla Wagner**: In each trial, subjects update the expected values of each option (Q), based on the history of previous outcome. Then, they use these values to make the next decision. Every now and then, they explore the low-value option (model 3 with 2 parameters: learning rate $\\alpha_{RW}$, softmax inverse temperature $\\beta_{RW}$)\n",
    "\n",
    "- **Choice kernel**: Subjects have a tendency to repeat responses, the strength of which depends on the history of actions they took before the current trial, which is tracked in the \"choice kernel\" (model 4 with 2 parameters, choice-kernel learning rate $\\alpha_{CK}$, choice-kernel inverse temperature $\\beta_{CK}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initialize the models\n",
    "\n",
    "The models are already defined in `models.py`. Let's import and initialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WinStayLoseSwitch, RescorlaWagner, ChoiceKernel\n",
    "\n",
    "# Initialize models\n",
    "wsls_model = WinStayLoseSwitch()\n",
    "rw_model = RescorlaWagner()\n",
    "ck_model = ChoiceKernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. (Optional) Implement Rescorla Wagner\n",
    "\n",
    "While all models are coded up in `models.py`, try defining the Rescorla Wagner model yourself. Take a look at the structure of the other models in `models.py` and especially the abstract base class which defines the structure of the models `RLModel`. \n",
    "\n",
    "Now, fill out the relevant methods in the model below and see how your definitions compares with the one in `models.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf85424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RLModel\n",
    "\n",
    "class RescorlaWagner(RLModel):\n",
    "    def simulate(self, T, mu, alpha, beta):\n",
    "        # initialize Q values, actions, rewards\n",
    "        Q = np.array([0.5, 0.5])\n",
    "        a, r = [], []\n",
    "\n",
    "        # for each timestep:\n",
    "        #   compute choice probability, action, reward\n",
    "        #   append action and reward to respective lists\n",
    "        #   update the Q value\n",
    "        # return the list of actions and rewards\n",
    "\n",
    "        return np.array(a), np.array(r)\n",
    "\n",
    "    def likelihood(self, pars, a, r):\n",
    "        alpha, beta = pars\n",
    "        Q = np.array([0.5, 0.5])\n",
    "        choice_p = []\n",
    "\n",
    "        # for each timestep:\n",
    "        #   compute action probability\n",
    "        #   append it to the choice_p list\n",
    "        #   update the Q value\n",
    "        # return the negative log likelihood\n",
    "\n",
    "    def initial_parameters(self):\n",
    "        return [np.random.random(), np.random.exponential()]\n",
    "\n",
    "    def parameter_bounds(self):\n",
    "        return [(0, 1), (0, np.inf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Simulate the models\n",
    "\n",
    "Next, let's write a function to simulate the models. Once the function is ready, simulate the models we initialized in section 2.1. Use these parameters for the models:\n",
    "\n",
    "- Win Stay Lose Shift: $\\epsilon=0.1$\n",
    "- Rescorla Wagner: $\\alpha_{RW}=0.1$ and $\\beta_{RW}=3$\n",
    "- Choice Kernel: $\\alpha_{CK}=0.1$ and $\\beta_{CK}=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wsls_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m     actions, rewards \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# for each repitition\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#   simiulate models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#   append actions and rewards to the respective lists\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# return actions, rewards\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Simulate models\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m a_wsls, r_wsls \u001b[38;5;241m=\u001b[39m simulate_model(\u001b[43mwsls_model\u001b[49m, num_repetitions, num_trials, reward_probabilities, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     11\u001b[0m a_rw, r_rw \u001b[38;5;241m=\u001b[39m simulate_model(rw_model, num_repetitions, num_trials, reward_probabilities, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     12\u001b[0m a_ck, r_ck \u001b[38;5;241m=\u001b[39m simulate_model(ck_model, num_repetitions, num_trials, reward_probabilities, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wsls_model' is not defined"
     ]
    }
   ],
   "source": [
    "# define a function to simulate the models and collect data\n",
    "def simulate_model(model, num_reps, num_trials, reward_probs, **params):\n",
    "    actions, rewards = [], []\n",
    "    # for each repitition\n",
    "    #   simiulate models\n",
    "    #   append actions and rewards to the respective lists\n",
    "    # return actions, rewards\n",
    "\n",
    "# Simulate models\n",
    "a_wsls, r_wsls = simulate_model(wsls_model, num_repetitions, num_trials, reward_probabilities, epsilon=0.1)\n",
    "a_rw, r_rw = simulate_model(rw_model, num_repetitions, num_trials, reward_probabilities, alpha=0.1, beta=5)\n",
    "a_ck, r_ck = simulate_model(ck_model, num_repetitions, num_trials, reward_probabilities, alpha=0.1, beta=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, you should also play with the parameters used to generate the simulations and observe the effect on Win-Stay-Lose-Shift analysis (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Behavioral analyses: Visualise the behaviour of different simulated models.\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now observe the behavioral outcomes resulting from each simulation. A typical analysis that might allow you to compare qualitatively the model behavior with actual subjects playing the 2-armed bandit is a win-stay-lose-shift analysis that plots the probability of repeating an action, p(stay), as a function of the previous reward (see Wilson & Collins, box 2 figure 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Compare qualitative patterns from our three different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect win-stay-lose-shift behavior for all three models. Write a function that calculates $p(stay_{n} | win_{n-1} = 0)$ and $p(stay_{n} | win_{n-1} = 1)$ from a single sequence of simulated actions and rewards.\n",
    "\n",
    "Then, average probabilities across simulations for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stay_probabilities(actions, rewards):\n",
    "    \"\"\"Compute probability of stay given win/lose.\"\"\"\n",
    "    # return lose_stay, win_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WSLS for each model\n",
    "model_actions = [a_wsls, a_rw, a_ck]\n",
    "model_rewards = [r_wsls, r_rw, r_ck]\n",
    "model_names = ['WSLS', 'Rescorla-Wagner', 'Choice kernel']\n",
    "\n",
    "wsls_probs = []\n",
    "for actions, rewards in zip(model_actions, model_rewards):\n",
    "    wsls_probs.append(np.mean([compute_stay_probabilities(actions[n], rewards[n]) for n in range(num_repetitions)], axis=0))\n",
    "\n",
    "# Loop over your models\n",
    "A = [a_wsls, a_rw, a_ck]\n",
    "R = [r_wsls, r_rw, r_ck]\n",
    "\n",
    "wsls_probs = []\n",
    "for actions, rewards in zip(model_actions, model_rewards):\n",
    "    wsls_probs.append(np.mean([compute_stay_probabilities(actions[n], rewards[n]) for n in range(num_repetitions)], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot WSLS behavior as a function of previous reward (1 for rewarded, 0 for unrewarded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does each model modulate its probability of staying as a function of previous reward? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WSLS as a function of previous reward\n",
    "plt.figure(figsize=(4, 4), dpi=100)\n",
    "for i, prob in enumerate(wsls_probs):\n",
    "    plt.plot([0, 1], prob, 'o-', label=model_names[i])\n",
    "plt.xlabel('Previous reward')\n",
    "plt.ylabel('Probability of staying')\n",
    "plt.xticks([0, 1])\n",
    "plt.legend(frameon=False)\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the choice kernel model leads to a reward-independent $p(stay)$, because choice probabilities are calculated independently of the previous reward. All other models show outcome-modulated behavior, with the starkest differences for the WSLS simulation.\n",
    "\n",
    "*Take home message*: More broadly, these patterns of behavior can then be contrasted again actual behavioral data to inform about subjects' behavior. It is important to simulate your candidate models and plot their behavior before comparing them to actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Let's analyse the performance of Model 3: p(correct) analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Rescorla Wagner, we are now interested in how learning rate and softmax inverse temperate affect the probability of choosing the arm with the highest reward. \n",
    "\n",
    "We will repeatedly perform a grid search over different parameter values (~ 1000 simulations with 100 trials per grid point) and store the mean $p(correct)$ across trials for each grid point and repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid search for Rescorla-Wagner model\n",
    "alphas = np.linspace(0.02, 1, 4)\n",
    "betas = np.array([1, 2, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first use only 10 simulations for each parameter combination. When your code works, increase to 1000.\n",
    "num_reps = 1_000\n",
    "\n",
    "# Initialize arrays to collect data\n",
    "correct = np.zeros((len(alphas), len(betas), num_reps))\n",
    "correct_early = np.zeros((len(alphas), len(betas), num_reps))\n",
    "correct_late = np.zeros((len(alphas), len(betas), num_reps))\n",
    "\n",
    "# Evaluation loop: grid-search over alpha and beta parameters for a large number of simulations\n",
    "# on which you will then average.\n",
    "    # simulate rescorla wagner model and collect sequence of actions and rewards\n",
    "    # collect the performance\n",
    "    # store performance for early (first 10 trials) and late (last 10 trials of a block) trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot $p(correct)$ as a function of $\\alpha$ and $\\beta$. Create the figure with two subplots: one for early, one for late trials.\n",
    "\n",
    "As in Wilson & Collins box 2 figure 1, plot different levels of $\\alpha$ on the x-axis and use different curves for $\\beta$ levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your figure with two subplots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does performance change as a function of alpha and beta parameter values, for early and late trials?\n",
    "\n",
    "The left graph shows that the learning rate is positively correlated with increases in early performance only for low $\\beta$ values, or very low $\\alpha$ values. For high $\\beta$ values, there is a U-shaped relationship between learning rate and early speed of learning. The right graph shows that with high $\\beta$ values, high learning rates negatively influence asymptotic behavior. Thus, both parameters interact to influence both the speed of learning and asymptotic performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion*: This kind of analysis will allow you to see qualitative differences between models, so that making their predictions in the experimental setup different. If the behavior of different models is not qualitatively different, this is a sign that you should try to design a better experiment. While not always possible, distinguishing between models on the basis of qualitative patterns in the data is preferable to quantitative model comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Simulation and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate Model 3 to generate synthetic data, then fit these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this step by step. We will first simulate actions of the model for a given learning rate, $\\alpha$, and softmax parameter, $\\beta$.\t\t\t\n",
    "\n",
    "After simulating the model, we will fit the parameters using a maximum likelihood approach to get estimated values $\\hat{\\alpha}$ and $\\hat{\\beta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "num_reps = 100\n",
    "alphas = np.random.rand(num_reps)\n",
    "betas = 10 * np.random.exponential(size=num_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the simulated data. \n",
    "\n",
    "Fit Rescorla Wagner by calling the `fit()` method for that model, `rw_model.fit()`, which takes as inputs actions and rewards. We will not focus on the goodness of fit here, but you should take a moment to look at the specification in `.fit()` and `likelihood()` methods of the model in `models.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random parameters\n",
    "\n",
    "# loop over repetitions \n",
    "\n",
    "    # set different fixed seeds per repetition\n",
    "    \n",
    "    # simulate M3\n",
    "\n",
    "    # fit and store parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Parameter recovery plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to visualise the fitted parameter values as a function of the generating parameter values. We will have one data point for each iteration. Here, you should use the generating values and the fitted values that you have stored in the section above. You should create two subplots, one for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your figure with two subplots goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you observe a fairly good agreement between the simulated and fit parameter values?\n",
    "\n",
    "The plot makes any correlations clear, and also reveals whether the correlation holds in some parameter regimes but not others. It also reveals any existing bias (e.g. a tendency to recover higher or lower values in average).\n",
    "\n",
    "Here we can see that the fit for $\\beta$ is best with a range, $0.1 < \\beta < 10$ and that outside this range, the correspondence between simulation and fit is not as good.\n",
    "\n",
    "Depending on the values of $\\beta$ that we obtain when fitting human behavior, this worse correspondence at small and large $\\beta$ may or may not be problematic. It may be a good idea to use the range of parameters obtained from fitting the real data to test the quality of recovery within the range that matters. \n",
    "\n",
    "Reliable parameter recovery is particularly important for look at inter-individual differences in relation to questionnaire scores or brain data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model recovery: confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate model recovery, here we will simulate behavior of our three models on the two-armed bandits task. \n",
    "\n",
    "As before, the means $\\mu$ can be set at 0.2 and 0.8, and the number of trials at $T = 1000$. For each simulation, model parameters can be sampled randomly for each model. \n",
    "\n",
    "Each simulated data set will then be fit to each of the given models, to determine which model fit best (according to BIC). This process will be repeated 100 times (number of \"repetitions\" or \"counts\") to compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "num_trials = 1_000\n",
    "num_counts = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a confusion matrix. It quantifies the probability that each model is the best fit to data generated from the other models. In a perfect world the confusion matrix will be the identity matrix, but in practice this is not always the case.\n",
    "\n",
    "How to read the confusion matrix? Given a winning model (a particular column), it tells you the likelihood of each ground-truth model (each row) to have generated the data (basically, the columns are more important than the rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# models to loop through\n",
    "models = [wsls_model, rw_model, ck_model]\n",
    "\n",
    "# initialise your confusion matrix: 3 by 3 (for our three models).\n",
    "confusion_matrix = np.zeros((3, 3))\n",
    "\n",
    "# Let's loop over number of repetitions: start with 10, increase to 100 if everything works\n",
    "\n",
    "    # set different fixed seed for each repetition\n",
    "\n",
    "    # for each model\n",
    "    #   simulate to get actions and rewards\n",
    "\n",
    "    # fit models\n",
    "    # compute best model and confusion matrix    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and print values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does your confusion matrix have large off-diagonal components? If so, this indicates that you have a problem with model recovery.\n",
    "\n",
    "- If you change the generating parameters, in particular the softmax temperature, do you get a better confusion matrix?\n",
    "\n",
    "You can observe then that the confusion matrix can be so dependent on the simulating parameter values means that it is crucial to match the simulation parameters to the actual fit parameters of your actual behavioral data (when you have some) as best as possible. Models that are identifiable in one parameter regime may be impossible to distinguish in another regime.\n",
    "\n",
    "A final note to remember: As with all model comparisons, it only tells you which model fits best of the models you considered. In and of itself, this is rather limited information as there are infinitely many other models that you did not consider. This makes it imperative to start with a good set of models initially, that rigorously capture the competing hypotheses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
