{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import matplotlib.cm as cmap\n",
    "import os\n",
    "\n",
    "# Download the custom library - do this manually if not running on google colab\n",
    "if not os.path.isfile(\"RL_library.py\"):\n",
    "    try:\n",
    "        from pydrive.auth import GoogleAuth\n",
    "        from pydrive.drive import GoogleDrive\n",
    "        from google.colab import auth\n",
    "        from oauth2client.client import GoogleCredentials\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please download RL_library.py manually from the github\")\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    _f = drive.CreateFile({'id': \"1Mn9GLnUkgCc2deBna2X9-uAy_wjkuX-g\"})\n",
    "    _f.GetContentFile('RL_library.py')\n",
    "\n",
    "# custom package\n",
    "from RL_library import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is related to:\n",
    "\n",
    "Bob Wilson & Anne Collins (2019) eLife: Ten simple rules for the computational modeling of behavioral data.\n",
    "\n",
    "In this paper, the authors describe a 2-armed bandit, where a player performs $T$ choices between two options. The machine has asymmetric reward probabilities $\\mu_{1} = 0.2$ and $\\mu_{2} = 0.8$ associated with each arm, which are initially unknown to the player.\n",
    "\n",
    "We want to find out: Which strategy does a subject use to maximize their overall reward on this machine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define our experiment parameters to simulate data from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "T   = 100                    # number of trials\n",
    "mu  = np.array([0.2, 0.8])   # mean reward of each bandit\n",
    "\n",
    "Nrep = 110 # number of repetitions of the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate our candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider three different normative models (M2, M3, and M4) of how subjects might attack the problem of maximizing reward on a 2-armed bandit. You can read the mathematical formulation of each model in the paper. Shortly, the proposed models are:\n",
    "\n",
    "- **Noisy win-stay-lose-shift**: Rewarded actions are repeated, unrewarded actions lead to the player choosing a different arm in the next trial. Every now and then, the subject explores a different option (model 2 with 1 parameter, $\\epsilon$, describing the overall level of randomness)\n",
    "\n",
    "- **Rescorla Wagner**: In each trial, subjects update the expected values of each option (Q), based on the history of previous outcome. Then, they use these values to make the next decision. Every now and then, they explore the low-value option (model 3 with 2 parameters: learning rate $\\alpha_{RW}$, softmax inverse temperature $\\beta_{RW}$)\n",
    "\n",
    "- **Choice kernel**: Subjects have a tendency to repeat responses, the strength of which depends on the history of actions they took before the current trial, which is tracked in the \"choice kernel\" (model 4 with 2 parameters, choice-kernel learning rate $\\alpha_{CK}$, choice-kernel inverse temperature $\\beta_{CK}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Win-stay-lose-shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate Model 2, the win-stay-lose-shift (M2WSLS) by calling `simulate_M2WSLS_v1` with $\\epsilon=0.1$, which is coded up in `RL_library.py`. \n",
    "\n",
    "This simulation process (as well as all the others in part 2. of the tutorial) should be repeated several times (`Nrep`), with different parameter settings, to get a handle on how the model behaves as a function of its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Win-stay-lose-shift\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Rescorla Wagner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While all models are coded up in `RL_library.py`, try out yourself to write a function for simulating Rescorla Wagner simulate_M3RescorlaWagner_v1 for Model 3 (code instructions below). If you are stuck, have a look at the implementation in the library.\n",
    "\n",
    "Then, simulate this model with $\\alpha_{RW} = 0.1$ and $\\beta_{RW} = 5$ and collect resulting actions and rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_M3RescorlaWagner_v1(T, mu, alpha, beta):\n",
    "# learning rate alpha, softmax parameter beta\n",
    "\n",
    "    # initialise Q values for each of the two options: initially, no preference\n",
    "\n",
    "    # loop over trials\n",
    "\n",
    "        # compute choice probabilities using softmax formula\n",
    "\n",
    "        # save actions and rewards\n",
    "        \n",
    "        # update values using prediction error term\n",
    "\n",
    "#     return a, r # return sequence of actions and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Rescorla Wagner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Choice kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, simulate a choice kernel model by calling `simulate_M4ChoiceKernel_v1` with $\\alpha_{CK}=0.1$ and $\\beta_{CK}=3$ (model 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Choice kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Rescorla-Wagner + choice kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, you should also play with the parameters used to generate the simulations and observe the effect on Win-Stay-Lose-Shift analysis (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Behavioral analyses: Visualise the behaviour of different simulated models.\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now observe the behavioral outcomes resulting from each simulation. A typical analysis that might allow you to compare qualitatively the model behavior with actual subjects playing the 2-armed bandit is a win-stay-lose-shift analysis that plots the probability of repeating an action, p(stay), as a function of the previous reward (see Wilson & Collins, box 2 figure 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Compare qualitative patterns from our three different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect win-stay-lose-shift behavior for all three models. Write a function that calculates $p(stay_{n} | win_{n-1} = 0)$ and $p(stay_{n} | win_{n-1} = 1)$ from a single sequence of simulated actions and rewards.\n",
    "\n",
    "Then, average probabilities across simulations for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wsls(a, r):\n",
    "\n",
    "#     return loseStay, winStay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over your models\n",
    "\n",
    "names = ['WSLS', 'Rescorla-Wagner', 'Choice kernel']\n",
    "\n",
    "# calculate WSLS with shape (3 x 2), # n_models x n_prev_outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot WSLS behavior as a function of previous reward (1 for rewarded, 0 for unrewarded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does each model modulate its probability of staying as a function of previous reward? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WSLS as a function of previous reward (1 for rewarded, 0 for unrewarded).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the choice kernel model leads to a reward-independent $p(stay)$, because choice probabilities are calculated independently of the previous reward. All other models show outcome-modulated behavior, with the starkest differences for the WSLS simulation.\n",
    "\n",
    "*Take home message*: More broadly, these patterns of behavior can then be contrasted again actual behavioral data to inform about subjects' behavior. It is important to simulate your candidate models and plot their behavior before comparing them to actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Let's analyse the performance of Model 3: p(correct) analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Rescorla Wagner, we are now interested in how learning rate and softmax inverse temperate affect the probability of choosing the arm with the highest reward. \n",
    "\n",
    "We will repeatedly perform a grid search over different parameter values (~ 1000 simulations with 100 trials per grid point) and store the mean $p(correct)$ across trials for each grid point and repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Define a range of parameter values to loop over for each parameter.\n",
    "alphas = np.linspace(0.02, 1, 20)\n",
    "betas  = np.array([1, 2, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first use only 10 simulations for each parameter combination. When your code works, increase to 1000.\n",
    "Nreps = 10\n",
    "\n",
    "# Initialization\n",
    "\n",
    "# Grid-search over alpha and beta parameters for a large number of simulations\n",
    "# on which you will then average.\n",
    "\n",
    "    # simulate Model 3 and collect sequence of actions and rewards\n",
    "\n",
    "    # collect the performance\n",
    "\n",
    "    # store performance for early (first 10 trials) and late (last 10 trials of a block) trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot $p(correct)$ as a function of $\\alpha$ and $\\beta$. Create the figure with two subplots: one for early, one for late trials.\n",
    "\n",
    "As in Wilson & Collins box 2 figure 1, plot different levels of $\\alpha$ on the x-axis and use different curves for $\\beta$ levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your figure with two subplots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does performance change as a function of alpha and beta parameter values, for early and late trials?\n",
    "\n",
    "The left graph shows that the learning rate is positively correlated with increases in early performance only for low $\\beta$ values, or very low $\\alpha$ values. For high $\\beta$ values, there is a U-shaped relationship between learning rate and early speed of learning. The right graph shows that with high $\\beta$ values, high learning rates negatively influence asymptotic behavior. Thus, both parameters interact to influence both the speed of learning and asymptotic performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion*: This kind of analysis will allow you to see qualitative differences between models, so that making their predictions in the experimental setup different. If the behavior of different models is not qualitatively different, this is a sign that you should try to design a better experiment. While not always possible, distinguishing between models on the basis of qualitative patterns in the data is preferable to quantitative model comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Simulation and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate Model 3 to generate synthetic data, then fit these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this step by step. We will first simulate actions of the model for a given learning rate, $\\alpha$, and softmax parameter, $\\beta$.\t\t\t\n",
    "\n",
    "After simulating the model, we will fit the parameters using a maximum likelihood approach to get estimated values $\\hat{\\alpha}$ and $\\hat{\\beta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the simulated data. \n",
    "\n",
    "Fit Rescorla Wagner by calling `fit_M3RescorlaWagner_v1`. As an input, enter simulated actions and rewards. As an output, extract the fitted parameter values, the loglikelihood LL and the BIC.\n",
    "\n",
    "We will not focus on the goodness of fit here, but you should take a moment to look at the specification in `fit_M3RescorlaWagner_v1` and `lik_M3RescorlaWagner_v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random parameters\n",
    "     # learning rate from a uniform distribution between 0 and 1\n",
    "     # softmax parameter from exponential with mean = 10\n",
    "\n",
    "# loop over repetitions \n",
    "\n",
    "    # set different fixed seeds per repetition\n",
    "    \n",
    "    # simulate M3\n",
    "\n",
    "    # fit and store parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Parameter recovery plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to visualise the fitted parameter values as a function of the generating parameter values. We will have one data point for each iteration. Here, you should use the generating values and the fitted values that you have stored in the section above. You should create two subplots, one for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your figure with two subplots goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you observe a fairly good agreement between the simulated and fit parameter values?\n",
    "\n",
    "The plot makes any correlations clear, and also reveals whether the correlation holds in some parameter regimes but not others. It also reveals any existing bias (e.g. a tendency to recover higher or lower values in average).\n",
    "\n",
    "Here we can see that the fit for $\\beta$ is best with a range, $0.1 < \\beta < 10$ and that outside this range, the correspondence between simulation and fit is not as good.\n",
    "\n",
    "Depending on the values of $\\beta$ that we obtain when fitting human behavior, this worse correspondence at small and large $\\beta$ may or may not be problematic. It may be a good idea to use the range of parameters obtained from fitting the real data to test the quality of recovery within the range that matters. \n",
    "\n",
    "Reliable parameter recovery is particularly important for look at inter-individual differences in relation to questionnaire scores or brain data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model recovery: confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate model recovery, here we will simulate behavior of our three models on the two-armed bandits task. \n",
    "\n",
    "As before, the means $\\mu$ can be set at 0.2 and 0.8, and the number of trials at $T = 1000$. For each simulation, model parameters can be sampled randomly for each model. \n",
    "\n",
    "Each simulated data set will then be fit to each of the given models, to determine which model fit best (according to BIC). This process will be repeated 100 times (number of \"repetitions\" or \"counts\") to compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters mu and T\n",
    "mu = np.array([.2, .8])\n",
    "T  = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a confusion matrix. It quantifies the probability that each model is the best fit to data generated from the other models. In a perfect world the confusion matrix will be the identity matrix, but in practice this is not always the case.\n",
    "\n",
    "How to read the confusion matrix? Given a winning model (a particular column), it tells you the likelihood of each ground-truth model (each row) to have generated the data (basically, the columns are more important than the rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make list of fitting functions to loop through\n",
    "funs = [fit_M2WSLS_v1, fit_M3RescorlaWagner_v1, fit_M4ChoiceKernel_v1]\n",
    "\n",
    "# initialise your confusion matrix: 3 by 3 (for our three models).\n",
    "\n",
    "# Let's loop over number of repetitions: start with 10, increase to 100 if everything works\n",
    "\n",
    "    # set different fixed seed for each repetition\n",
    "\n",
    "    #  simulate Model 2 with random epsilon parameter (sampled from uniform [0,1])\n",
    "    # fit models\n",
    "\n",
    "    #  simulate Model 3 with random alpha and beta parameters\n",
    "    # initialize from uniform [0,1] and 1+exponential with mean 1\n",
    "    # fit models\n",
    "    \n",
    "    #  simulate Model 4 with random alpha and beta parameters\n",
    "    # initialize from uniform [0,1] and 1+ exponential with mean 1\n",
    "    # fit models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and print values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does your confusion matrix have large off-diagonal components? If so, this indicates that you have a problem with model recovery.\n",
    "\n",
    "- If you change the generating parameters, in particular the softmax temperature, do you get a better confusion matrix?\n",
    "\n",
    "You can observe then that the confusion matrix can be so dependent on the simulating parameter values means that it is crucial to match the simulation parameters to the actual fit parameters of your actual behavioral data (when you have some) as best as possible. Models that are identifiable in one parameter regime may be impossible to distinguish in another regime.\n",
    "\n",
    "A final note to remember: As with all model comparisons, it only tells you which model fits best of the models you considered. In and of itself, this is rather limited information as there are infinitely many other models that you did not consider. This makes it imperative to start with a good set of models initially, that rigorously capture the competing hypotheses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
