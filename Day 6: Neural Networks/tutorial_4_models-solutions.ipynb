{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward models: Perceptron and Multi-Layer Perceptron\n",
    "\n",
    "In this tutorial, we will learn how to use Perceptron and Multi-Layer Perceptron (MLP) neural networks to solve simple classification tasks. We will walk through the necessary steps to implement and train these networks using PyTorch.\n",
    "\n",
    "## Index\n",
    "1. Imports\n",
    "2. Models\n",
    "    - Perceptron\n",
    "    - Multi-Layer Perceptron\n",
    "2. Generating data\n",
    "3. Training\n",
    "4. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The Perceptron is one of the simplest types of artificial neural networks. It consists of a single layer of neurons, each with a set of weights and biases. The output of the Perceptron is calculated as the weighted sum of the inputs plus the bias, passed through an activation function.\n",
    "\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b$$\n",
    "\n",
    "$$y = \\gamma(z)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{w}$ is the weight vector\n",
    "- $\\mathbf{x}$ is the input vector\n",
    "- $b$ is the bias\n",
    "- $\\gamma$ is the sigmoid function.\n",
    "- $\\mathbf{z}$ is called the logit.\n",
    "\n",
    "To implement the perceptron in PyTorch, we can create a class that inherits from `torch.nn.Module` and the define the different elements of the network in the constructor and the forward pass in the `forward` method. You can use the matrix multiplication operation `torch.matmul` to calculate the weighted sum of the inputs and the bias. \n",
    "\n",
    "[Note: although the equations above use the sigmoid function as the activation function, we will not apply it within the model. Instead, we will apply it later when calculating the loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Perceptron, self).__init__()\n",
    "        # INSTRUCTION 1: set the weights and bias of the perceptron\n",
    "        self.W = nn.Parameter(torch.randn(output_size, input_size) * 0.01)\n",
    "        self.b = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # INSTRUCTION 2: implement the forward pass of the perceptron\n",
    "        x = torch.matmul(x, self.W.T) + self.b\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "THe multi-layer perceptron (MLP) is a type of feedforward neural network that consists of multiple layers of neurons. Each layer is fully connected to the next layer. The formula to calculate the output of an MLP is similar to the perceptron, but with the addition of non-linear activation functions between the layers:\n",
    "\n",
    "$$z^{(1)} = \\mathbf{W}^{(1)} \\cdot \\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$h^{(1)} = \\phi(z^{(1)})$$\n",
    "$$z^{(2)} = \\mathbf{W}^{(2)} \\cdot \\mathbf{h}^{(1)} + \\mathbf{b}^{(2)}$$\n",
    "$$y = \\gamma(z^{(2)})$$\n",
    "\n",
    "Where:\n",
    "- x is the input vector.\n",
    "- $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$ are the weight matrices of the first and second layers.\n",
    "- $\\mathbf{b}^{(1)}$ and $\\mathbf{b}^{(2)}$ are the bias vectors of the first and second layers.\n",
    "- $\\phi$ is the activation function. In this case, we will use the ReLU activation function.\n",
    "- $\\gamma$ is the output activation function. In this case, we will use the sigmoid activation function.\n",
    "- $\\mathbf{z}$ is called the logit.\n",
    "\n",
    "\n",
    "Note that if we don't include any non-linear activation functions, the MLP is equivalent to a linear regression model. However, by adding non-linear activation functions, the MLP can learn complex patterns in the data. \n",
    "\n",
    "[Note: although the equations above use the sigmoid function as the activation function, we will not apply it within the model. Instead, we will apply it later when calculating the loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, non_linear_activation='relu'):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.nla = non_linear_activation\n",
    "        # INSTRUCTION 3: set the weights and bias of the MLP\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)\n",
    "        self.b_ih = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.W_ho = nn.Parameter(torch.randn(output_size, hidden_size) * 0.01)\n",
    "        self.b_ho = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.nla=='':\n",
    "            # INSTRUCTION 4: apply the first linear layer with no activation function\n",
    "            h = torch.matmul(x, self.W_ih.T) + self.b_ih\n",
    "        elif self.nla=='relu':\n",
    "            # INSTRUCTION 5: apply a ReLU activation function to the hidden layer\n",
    "            h = torch.relu(torch.matmul(x, self.W_ih.T) + self.b_ih)\n",
    "        elif self.nla=='sigmoid':\n",
    "            # INSTRUCTION 6: apply a sigmoid activation function to the hidden layer\n",
    "            h = torch.sigmoid(torch.matmul(x, self.W_ih.T) + self.b_ih)\n",
    "        \n",
    "        # INSTRUCTION 7: apply the second linear layer\n",
    "        x = torch.matmul(h, self.W_ho.T) + self.b_ho\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "\n",
    "We will now generate the data for classification. We will generate two types of data: linearly separable data and non-linearly separable data.\n",
    "We will also plot the data to visualize it. You will see that the plotting function can call the evaluation function to plot the decision boundary of the model. We will implement this function in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, Y, title, model=None):\n",
    "    \"\"\"\n",
    "    Function to plot the decision boundary and data points of a model\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    if model is not None:\n",
    "        Z = evaluate_model(model, grid) \n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def generate_linear_data(N=100, D=2):\n",
    "    \"\"\"\n",
    "    Function to generate linearly separable data\n",
    "    \"\"\"\n",
    "    X = np.random.randn(N, D)\n",
    "    X[:N//2, :] += 1\n",
    "    X[N//2:, :] -= 1\n",
    "    Y = np.concatenate((np.zeros(N//2), np.ones(N//2)))\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "def generate_xor_data(n_points=100):\n",
    "    \"\"\"\n",
    "    Function to generate XOR data\n",
    "    \"\"\"\n",
    "    # Base XOR points\n",
    "    base_points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "    base_labels = np.array([0, 1, 1, 0], dtype=np.float32)\n",
    "    \n",
    "    # Generate more points around the base points\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _ in range(n_points // 4):\n",
    "        for point, label in zip(base_points, base_labels):\n",
    "            noise = np.random.normal(0, 0.1, size=point.shape)\n",
    "            X.append(point + noise)\n",
    "            Y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "# Task 1: Linearly Separate Two Clouds of Dots\n",
    "X_linear, Y_linear = generate_linear_data()\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "plot_decision_boundary(X_linear, Y_linear, 'Linear Data')\n",
    "X_xor, Y_xor = generate_xor_data()\n",
    "plot_decision_boundary(X_xor, Y_xor, 'XOR Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now implement the training and evaluating functions. For the training function, we will use the binary cross-entropy loss with logits, which combines the sigmoid activation function and the binary cross-entropy loss. We will use the Stoachastic Gradient Descent (SGD) optimizer to update the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,  X, Y, num_epochs=1000, print_interval=100, lr=0.1):\n",
    "    # INSTRUCTION 8: define the loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # INSTRUCTION 9: define the optimizer setting the learning rate\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # INSRTUCTION 10: get the model's output\n",
    "        outputs = model(X)\n",
    "        # INSTRUCTION 11: calculate the loss\n",
    "        loss = criterion(outputs, Y.float().view(-1, 1))\n",
    "        # INSTRUCTION 12: backpropagate the loss\n",
    "        loss.backward()\n",
    "        # INSTRUCTION 13: update the model's parameters\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % print_interval == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def evaluate_model(model, grid):\n",
    "    with torch.no_grad():\n",
    "        # INSTRUCTION 14: get the model's output\n",
    "        Z = model(grid)\n",
    "        # INSRTUCTION 15: apply the sigmoid function to the output\n",
    "        Z = torch.sigmoid(Z).numpy()\n",
    "        # INSTRUCTION 16: binarize the output\n",
    "        Z = (Z > 0.5).astype(int)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models\n",
    "\n",
    "Let's test the models you have implemented. We will train the models on the linearly separable data and non-linearly separable data and evaluate their performance. We will also plot the decision boundary of the models to visualize how they separate the data.\n",
    "Things you can play with:\n",
    "- The number of neurons in the hidden layer of the MLP.\n",
    "- The learning rate of the optimizer.\n",
    "- The activation function of the hidden layer of the MLP.\n",
    "- The number of epochs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2  # number of neurons in the hidden layer\n",
    "lr = 0.1  # learning rate\n",
    "non_linear_activation = 'relu'  # activation function for the hidden layer\n",
    "num_epochs = 1000  # number of training epochs\n",
    "\n",
    "# Task 1: Linearly Separate Two Clouds of Dots\n",
    "X_linear, Y_linear = generate_linear_data()\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# Perceptron\n",
    "model_perceptron = Perceptron(input_size, output_size)\n",
    "train_model(model_perceptron, X_linear, Y_linear, num_epochs=num_epochs, lr=lr, print_interval=100)\n",
    "plot_decision_boundary(X_linear, Y_linear, 'Perceptron - Linear Data', model=model_perceptron)\n",
    "\n",
    "# MLP\n",
    "model_mlp = MLP(input_size, hidden_size, output_size,\n",
    "                 non_linear_activation=non_linear_activation)\n",
    "train_model(model_mlp, X_linear, Y_linear, num_epochs=num_epochs, lr=lr, print_interval=100)\n",
    "plot_decision_boundary(X_linear, Y_linear, 'MLP - Linear Data', model=model_mlp)\n",
    "\n",
    "# Task 2: XOR Task\n",
    "X_xor, Y_xor = generate_xor_data()\n",
    "\n",
    "# Perceptron\n",
    "model_perceptron = Perceptron(input_size, output_size)\n",
    "train_model(model_perceptron, X_xor, Y_xor, num_epochs=num_epochs, lr=lr, print_interval=1000)\n",
    "plot_decision_boundary(X_xor, Y_xor, 'Perceptron - XOR Data', model=model_perceptron)\n",
    "\n",
    "# MLP\n",
    "model_mlp = MLP(input_size, hidden_size, output_size,\n",
    "                 non_linear_activation=non_linear_activation)\n",
    "train_model(model_mlp, X_xor, Y_xor, num_epochs=num_epochs, lr=lr, print_interval=1000)\n",
    "plot_decision_boundary(X_xor, Y_xor, 'MLP - XOR Data', model=model_mlp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
